import json
import logging
from datetime import datetime
from typing import Annotated, Any

from fastmcp import Context, FastMCP
from pydantic import Field
from qdrant_client import models

from mcp_server_qdrant.common.filters import make_indexes
from mcp_server_qdrant.common.func_tools import make_partial_function
from mcp_server_qdrant.common.wrap_filters import wrap_filters
from mcp_server_qdrant.embeddings.factory import create_embedding_provider
from mcp_server_qdrant.qdrant import ArbitraryFilter, Entry, Metadata, QdrantConnector
from mcp_server_qdrant.settings import (
    EmbeddingProviderSettings,
    QdrantSettings,
    ToolSettings,
)
from mcp_server_qdrant.system_monitor import UniversalQdrantMonitor
from mcp_server_qdrant.storage_optimizer import QdrantStorageOptimizer
from mcp_server_qdrant.ragbridge.connector import RAGBridgeConnector
from mcp_server_qdrant.ragbridge.models import ContentType, SearchContext, RAGEntry, RAGMetadata
from mcp_server_qdrant.ragbridge.vocabulary_api import vocabulary_api
from mcp_server_qdrant.ragbridge.fragment_manager import fragment_manager
from mcp_server_qdrant.ragbridge.schema_api import schema_api
from mcp_server_qdrant.ragbridge.schema_approval import get_approval_manager
from mcp_server_qdrant.permission_manager import get_permission_manager, PermissionLevel
from mcp_server_qdrant.data_migration_tool import DataMigrationTool

logger = logging.getLogger(__name__)


# FastMCP is an alternative interface for declaring the capabilities
# of the server. Its API is based on FastAPI.
class QdrantMCPServer(FastMCP):
    """
    A MCP server for Qdrant.
    """

    def __init__(
        self,
        tool_settings: ToolSettings,
        qdrant_settings: QdrantSettings,
        embedding_provider_settings: EmbeddingProviderSettings,
        name: str = "mcp-server-qdrant",
        instructions: str | None = None,
        **settings: Any,
    ):
        self.tool_settings = tool_settings
        self.qdrant_settings = qdrant_settings
        self.embedding_provider_settings = embedding_provider_settings

        self.embedding_provider = create_embedding_provider(embedding_provider_settings)
        self.qdrant_connector = QdrantConnector(
            qdrant_settings.location,
            qdrant_settings.api_key,
            qdrant_settings.collection_name,
            self.embedding_provider,
            qdrant_settings.local_path,
            make_indexes(qdrant_settings.filterable_fields_dict()),
        )
        
        # Initialize RAG Bridge connector
        self.ragbridge_connector = RAGBridgeConnector(
            qdrant_url=qdrant_settings.location,
            qdrant_api_key=qdrant_settings.api_key,
            embedding_provider=self.embedding_provider,
            qdrant_local_path=qdrant_settings.local_path,
            default_collection_prefix="ragbridge",
        )
        
        # åˆå§‹åŒ–é€šç”¨ç³»çµ±ç›£æ§å™¨
        self.system_monitor = UniversalQdrantMonitor(
            self.qdrant_connector._client,
            qdrant_settings.location
        )
        
        # åˆå§‹åŒ–å„²å­˜å„ªåŒ–å·¥å…·
        self.storage_optimizer = QdrantStorageOptimizer(
            self.qdrant_connector._client
        )
        
        # åˆå§‹åŒ–æ¬Šé™ç®¡ç†ç³»çµ±
        self.permission_manager = get_permission_manager()
        if qdrant_settings.enable_permission_system:
            # è¨­å®šé è¨­æ¬Šé™ç´šåˆ¥
            default_level = PermissionLevel(qdrant_settings.default_permission_level)
            self.permission_manager.set_user_permission("default_user", default_level)
            logger.info(f"Permission system enabled with default level: {default_level.value}")
        else:
            # å‘å¾Œå…¼å®¹ï¼šå¦‚æœåœç”¨æ¬Šé™ç³»çµ±ï¼Œè¨­å®šç‚ºè¶…ç´šç®¡ç†å“¡
            self.permission_manager.set_user_permission("default_user", PermissionLevel.SUPER_ADMIN)
            logger.info("Permission system disabled - granting super_admin access")
        
        # åˆå§‹åŒ–è³‡æ–™é·ç§»å·¥å…·
        from mcp_server_qdrant.ragbridge.vocabulary import VocabularyManager
        vocabulary_manager = VocabularyManager()
        self.migration_tool = DataMigrationTool(
            qdrant_client=self.qdrant_connector._client,
            ragbridge_connector=self.ragbridge_connector,
            vocabulary_manager=vocabulary_manager
        )

        super().__init__(name=name, instructions=instructions, **settings)

        self.setup_tools()

    def format_entry(self, entry: Entry) -> str:
        """
        Feel free to override this method in your subclass to customize the format of the entry.
        """
        entry_metadata = json.dumps(entry.metadata) if entry.metadata else ""
        return f"<entry><content>{entry.content}</content><metadata>{entry_metadata}</metadata></entry>"
    
    def check_permission_wrapper(self, func, tool_name: str):
        """
        åŒ…è£å·¥å…·å‡½æ•¸ä»¥æ·»åŠ æ¬Šé™æª¢æŸ¥
        """
        async def permission_checked_func(ctx: Context, *args, **kwargs):
            # ç²å–ç”¨æˆ¶IDï¼ˆåœ¨å¯¦éš›ä½¿ç”¨ä¸­å¯èƒ½ä¾†è‡ª context æˆ–èªè­‰ç³»çµ±ï¼‰
            user_id = "default_user"  # ç›®å‰ä½¿ç”¨é è¨­ç”¨æˆ¶
            
            # æª¢æŸ¥æ¬Šé™
            if not self.permission_manager.check_tool_permission(user_id, tool_name):
                permission_level = self.permission_manager.get_user_permission(user_id)
                required_permission = self.permission_manager.tool_permissions.get(tool_name)
                required_level = required_permission.required_level.value if required_permission else "unknown"
                
                await ctx.debug(f"Permission denied: User {user_id} (level: {permission_level.value}) trying to access {tool_name} (requires: {required_level})")
                return [
                    f"âŒ **æ¬Šé™ä¸è¶³**",
                    f"ğŸ” å·¥å…·åç¨±: {tool_name}",
                    f"ğŸ‘¤ ç•¶å‰æ¬Šé™ç´šåˆ¥: {permission_level.value}",
                    f"âš ï¸ éœ€è¦æ¬Šé™ç´šåˆ¥: {required_level}",
                    "",
                    f"ğŸ’¡ **è§£æ±ºæ–¹æ¡ˆ:**",
                    f"è«‹è¯çµ¡ç®¡ç†å“¡æå‡æ¬Šé™ç´šåˆ¥ï¼Œæˆ–ä½¿ç”¨é©åˆæ‚¨æ¬Šé™çš„å·¥å…·ã€‚",
                    f"ä½¿ç”¨ 'get-user-permissions' å·¥å…·æŸ¥çœ‹å¯ç”¨çš„å·¥å…·åˆ—è¡¨ã€‚"
                ]
            
            # æ¬Šé™æª¢æŸ¥é€šéï¼ŒåŸ·è¡ŒåŸå§‹å‡½æ•¸
            return await func(ctx, *args, **kwargs)
        
        return permission_checked_func

    def setup_tools(self):
        """
        Register the tools in the server.
        """

        async def store(
            ctx: Context,
            information: Annotated[str, Field(description="Text to store")],
            collection_name: Annotated[
                str, Field(description="The collection to store the information in")
            ] = "default",
            # The `metadata` parameter is defined as non-optional, but it can be None.
            # If we set it to be optional, some of the MCP clients, like Cursor, cannot
            # handle the optional parameter correctly.
            metadata: Annotated[
                Metadata | None,
                Field(
                    description="Extra metadata stored along with memorised information. Any json is accepted."
                ),
            ] = None,
        ) -> str:
            """
            Store some information in Qdrant.
            :param ctx: The context for the request.
            :param information: The information to store.
            :param metadata: JSON metadata to store with the information, optional.
            :param collection_name: The name of the collection to store the information in, optional. If not provided,
                                    the default collection is used.
            :return: A message indicating that the information was stored.
            """
            await ctx.debug(f"Storing information {information} in Qdrant")

            entry = Entry(content=information, metadata=metadata)

            await self.qdrant_connector.store(entry, collection_name=collection_name)
            if collection_name:
                return f"Remembered: {information} in collection {collection_name}"
            return f"Remembered: {information}"

        async def find(
            ctx: Context,
            query: Annotated[str, Field(description="What to search for")],
            collection_name: Annotated[
                str, Field(description="The collection to search in")
            ] = "default",
            query_filter: ArbitraryFilter | None = None,
        ) -> list[str]:
            """
            Find memories in Qdrant.
            :param ctx: The context for the request.
            :param query: The query to use for the search.
            :param collection_name: The name of the collection to search in, optional. If not provided,
                                    the default collection is used.
            :param query_filter: The filter to apply to the query.
            :return: A list of entries found.
            """

            # Log query_filter
            await ctx.debug(f"Query filter: {query_filter}")

            query_filter = models.Filter(**query_filter) if query_filter else None

            await ctx.debug(f"Finding results for query {query}")

            entries = await self.qdrant_connector.search(
                query,
                collection_name=collection_name,
                limit=self.qdrant_settings.search_limit,
                query_filter=query_filter,
            )
            if not entries:
                return [f"No information found for the query '{query}'"]
            content = [
                f"Results for the query '{query}'",
            ]
            for entry in entries:
                content.append(self.format_entry(entry))
            return content

        find_foo = find
        store_foo = store

        filterable_conditions = (
            self.qdrant_settings.filterable_fields_dict_with_conditions()
        )

        if len(filterable_conditions) > 0:
            find_foo = wrap_filters(find_foo, filterable_conditions)
        elif not self.qdrant_settings.allow_arbitrary_filter:
            find_foo = make_partial_function(find_foo, {"query_filter": None})

        if self.qdrant_settings.collection_name:
            find_foo = make_partial_function(
                find_foo, {"collection_name": self.qdrant_settings.collection_name}
            )
            store_foo = make_partial_function(
                store_foo, {"collection_name": self.qdrant_settings.collection_name}
            )

        self.tool(
            find_foo,
            name="qdrant-find",
            description=self.tool_settings.tool_find_description,
        )

        if not self.qdrant_settings.read_only:
            # Those methods can modify the database
            self.tool(
                store_foo,
                name="qdrant-store",
                description=self.tool_settings.tool_store_description,
            )

            # æ–°å¢åˆªé™¤æ–‡æª”å·¥å…·
            async def delete_documents(
                ctx: Context,
                query: Annotated[str, Field(description="æœå°‹è¦åˆªé™¤çš„æ–‡æª”")],
                collection_name: Annotated[
                    str, Field(description="è¦åˆªé™¤æ–‡æª”çš„ collection åç¨±")
                ] = "default",
                confirm_delete: Annotated[
                    bool, Field(description="ç¢ºèªåˆªé™¤æ“ä½œï¼Œå¿…é ˆè¨­ç‚º True")
                ] = False,
                limit: Annotated[
                    int, Field(description="æœ€å¤šåˆªé™¤çš„æ–‡æª”æ•¸é‡")
                ] = 10,
            ) -> str:
                """
                åˆªé™¤ç¬¦åˆæœå°‹æ¢ä»¶çš„æ–‡æª”ã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚
                """
                await ctx.debug(f"Deleting documents matching '{query}' in collection '{collection_name}'")
                
                result = await self.qdrant_connector.delete_documents(
                    query=query,
                    collection_name=collection_name,
                    limit=limit,
                    confirm_delete=confirm_delete,
                )
                
                if "error" in result:
                    return f"éŒ¯èª¤: {result['error']}"
                
                return f"{result['message']} (åœ¨ collection '{result['collection_name']}')"

            # æ–°å¢åˆªé™¤ collection å·¥å…·
            async def delete_collection(
                ctx: Context,
                collection_name: Annotated[
                    str, Field(description="è¦åˆªé™¤çš„ collection åç¨±")
                ] = "default",
                confirm_delete: Annotated[
                    bool, Field(description="ç¢ºèªåˆªé™¤æ“ä½œï¼Œå¿…é ˆè¨­ç‚º True")
                ] = False,
            ) -> str:
                """
                åˆªé™¤æ•´å€‹ collection åŠå…¶æ‰€æœ‰æ–‡æª”ã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚
                """
                await ctx.debug(f"Deleting collection '{collection_name}'")
                
                result = await self.qdrant_connector.delete_collection(
                    collection_name=collection_name,
                    confirm_delete=confirm_delete,
                )
                
                if "error" in result:
                    return f"éŒ¯èª¤: {result['error']}"
                
                return result["message"]

            # æ–°å¢æ¬ç§»æ–‡æª”å·¥å…·
            async def move_documents(
                ctx: Context,
                query: Annotated[str, Field(description="æœå°‹è¦æ¬ç§»çš„æ–‡æª”")],
                source_collection: Annotated[
                    str, Field(description="ä¾†æº collection åç¨±")
                ] = "default",
                target_collection: Annotated[
                    str, Field(description="ç›®æ¨™ collection åç¨±")
                ] = "default",
                confirm_move: Annotated[
                    bool, Field(description="ç¢ºèªæ¬ç§»æ“ä½œï¼Œå¿…é ˆè¨­ç‚º True")
                ] = False,
                limit: Annotated[
                    int, Field(description="æœ€å¤šæ¬ç§»çš„æ–‡æª”æ•¸é‡")
                ] = 10,
            ) -> str:
                """
                æ¬ç§»æ–‡æª”å¾ä¸€å€‹ collection åˆ°å¦ä¸€å€‹ collectionã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚
                """
                await ctx.debug(f"Moving documents matching '{query}' from '{source_collection}' to '{target_collection}'")
                
                result = await self.qdrant_connector.move_documents(
                    query=query,
                    source_collection=source_collection,
                    target_collection=target_collection,
                    limit=limit,
                    confirm_move=confirm_move,
                )
                
                return f"{result['message']}"

            # æ–°å¢æ›´æ–° metadata å·¥å…·
            async def update_metadata(
                ctx: Context,
                query: Annotated[str, Field(description="æœå°‹è¦æ›´æ–°çš„æ–‡æª”")],
                new_metadata: Annotated[
                    dict, Field(description="æ–°çš„ metadataï¼Œæœƒèˆ‡ç¾æœ‰ metadata åˆä½µ")
                ],
                collection_name: Annotated[
                    str, Field(description="è¦æ›´æ–°æ–‡æª”çš„ collection åç¨±")
                ] = "default",
                confirm_update: Annotated[
                    bool, Field(description="ç¢ºèªæ›´æ–°æ“ä½œï¼Œå¿…é ˆè¨­ç‚º True")
                ] = False,
                limit: Annotated[
                    int, Field(description="æœ€å¤šæ›´æ–°çš„æ–‡æª”æ•¸é‡")
                ] = 10,
            ) -> str:
                """
                æ›´æ–°ç¬¦åˆæœå°‹æ¢ä»¶çš„æ–‡æª” metadataã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚
                """
                await ctx.debug(f"Updating metadata for documents matching '{query}' in collection '{collection_name}'")
                
                result = await self.qdrant_connector.update_metadata(
                    query=query,
                    new_metadata=new_metadata,
                    collection_name=collection_name,
                    limit=limit,
                    confirm_update=confirm_update,
                )
                
                if "error" in result:
                    return f"éŒ¯èª¤: {result['error']}"
                
                return result["message"]

            # æ–°å¢ç§»é™¤ metadata éµå·¥å…·
            async def remove_metadata_keys(
                ctx: Context,
                query: Annotated[str, Field(description="æœå°‹è¦è™•ç†çš„æ–‡æª”")],
                keys_to_remove: Annotated[
                    list[str], Field(description="è¦ç§»é™¤çš„ metadata éµåˆ—è¡¨")
                ],
                collection_name: Annotated[
                    str, Field(description="è¦è™•ç†æ–‡æª”çš„ collection åç¨±")
                ] = "default",
                confirm_update: Annotated[
                    bool, Field(description="ç¢ºèªæ›´æ–°æ“ä½œï¼Œå¿…é ˆè¨­ç‚º True")
                ] = False,
                limit: Annotated[
                    int, Field(description="æœ€å¤šè™•ç†çš„æ–‡æª”æ•¸é‡")
                ] = 10,
            ) -> str:
                """
                å¾ç¬¦åˆæœå°‹æ¢ä»¶çš„æ–‡æª” metadata ä¸­ç§»é™¤æŒ‡å®šçš„éµã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚
                """
                await ctx.debug(f"Removing metadata keys {keys_to_remove} from documents matching '{query}' in collection '{collection_name}'")
                
                result = await self.qdrant_connector.remove_metadata_keys(
                    query=query,
                    keys_to_remove=keys_to_remove,
                    collection_name=collection_name,
                    limit=limit,
                    confirm_update=confirm_update,
                )
                
                if "error" in result:
                    return f"éŒ¯èª¤: {result['error']}"
                
                return result["message"]

            # è¨»å†Šæ–°å·¥å…·
            self.tool(
                delete_documents,
                name="qdrant-delete-documents",
                description="åˆªé™¤ç¬¦åˆæœå°‹æ¢ä»¶çš„æ–‡æª”ã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚",
            )
            
            self.tool(
                delete_collection,
                name="qdrant-delete-collection", 
                description="åˆªé™¤æ•´å€‹ collection åŠå…¶æ‰€æœ‰æ–‡æª”ã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚",
            )
            
            self.tool(
                move_documents,
                name="qdrant-move-documents",
                description="æ¬ç§»æ–‡æª”å¾ä¸€å€‹ collection åˆ°å¦ä¸€å€‹ collectionã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚",
            )

            # é‡æ–°å•Ÿç”¨ï¼Œå·²ä¿®æ­£ PointStruct å‘é‡è³‡æ–™å•é¡Œ
            self.tool(
                update_metadata,
                name="qdrant-update-metadata",
                description="æ›´æ–°ç¬¦åˆæœå°‹æ¢ä»¶çš„æ–‡æª” metadataã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚",
            )
            
            self.tool(
                remove_metadata_keys,
                name="qdrant-remove-metadata-keys",
                description="å¾ç¬¦åˆæœå°‹æ¢ä»¶çš„æ–‡æª” metadata ä¸­ç§»é™¤æŒ‡å®šçš„éµã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚",
            )

        # æ–°å¢åˆ—å‡º collections å·¥å…·ï¼ˆåªè®€æ“ä½œï¼‰
        async def list_collections(ctx: Context) -> list[str]:
            """
            åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ collectionsã€‚
            """
            await ctx.debug("Listing all collections")
            collections = await self.qdrant_connector.list_collections()
            return collections

        self.tool(
            list_collections,
            name="qdrant-list-collections",
            description="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ collectionsã€‚",
        )

        # ç´” Qdrant API ç›£æ§å·¥å…·
        async def get_qdrant_status(ctx: Context) -> list[str]:
            """
            ç²å– Qdrant çš„ç‹€æ…‹è³‡è¨Šï¼Œåƒ…åŸºæ–¼ Qdrant è‡ªèº«çš„ APIã€‚
            """
            await ctx.debug("Getting Qdrant status via pure API")
            try:
                report = await self.system_monitor.get_comprehensive_analysis()
                
                # æ ¼å¼åŒ–å ±å‘Šç‚ºå¯è®€æ ¼å¼
                result = ["ğŸ” **Qdrant ç‹€æ…‹åˆ†æ (ç´” API ç‰ˆæœ¬)**"]
                result.append(f"ğŸ“… åˆ†ææ™‚é–“: {report['timestamp']}")
                result.append("")
                
                # éƒ¨ç½²ç’°å¢ƒè³‡è¨Š
                deployment = report.get('deployment_info', {})
                result.append("ğŸ—ï¸ **éƒ¨ç½²ç’°å¢ƒ**")
                result.append(f"   é¡å‹: {deployment.get('description', 'unknown')}")
                result.append(f"   ä¸»æ©Ÿ: {deployment.get('host', 'unknown')}")
                result.append(f"   ç«¯å£: {deployment.get('port', 'unknown')}")
                result.append(f"   å¯ç”¨åŠŸèƒ½: {', '.join(deployment.get('features', []))}")
                result.append("")
                
                # Qdrant å¥åº·ç‹€æ…‹
                health = report.get('health_status', {})
                result.append("ğŸ¥ **æœå‹™å¥åº·ç‹€æ…‹**")
                status_emoji = "âœ…" if health.get('status') == 'healthy' else "âŒ"
                result.append(f"   ç‹€æ…‹: {status_emoji} {health.get('status', 'unknown')}")
                if 'collections_count' in health:
                    result.append(f"   Collections: {health.get('collections_count', 0)} å€‹")
                if 'error' in health:
                    result.append(f"   éŒ¯èª¤: {health['error']}")
                result.append("")
                
                # Collections åˆ†æ
                collections_info = report.get('collections_info', {})
                if collections_info and 'collections' in collections_info:
                    collections = collections_info['collections']
                    summary = collections_info.get('summary', {})
                    
                    result.append("ğŸ“Š **Collections åˆ†æ**")
                    for collection in collections:
                        name = collection.get('name', 'unknown')
                        points = collection.get('points_count', 0)
                        vectors = collection.get('vectors_count', 0)
                        memory_mb = collection.get('estimated_memory_mb', 0)
                        status = collection.get('status', 'unknown')
                        
                        # æ•ˆèƒ½è©•ä¼°
                        perf_emoji = "ğŸŸ¢" if memory_mb < 100 else "ğŸŸ¡" if memory_mb < 500 else "ğŸ”´"
                        result.append(f"   ğŸ“ {name}: {points:,} points, {vectors:,} vectors, ~{memory_mb:.1f}MB {perf_emoji}")
                        result.append(f"      ç‹€æ…‹: {status}")
                        
                        # ç´¢å¼•ç‹€æ…‹
                        indexed = collection.get('indexed_vectors_count', 0)
                        if vectors > 0:
                            index_ratio = indexed / vectors
                            if index_ratio < 0.9:
                                result.append(f"      âš ï¸ ç´¢å¼•ç‡: {index_ratio:.1%} (å»ºè­°é‡æ–°ç´¢å¼•)")
                    
                    result.append("")
                    result.append("ğŸ“ˆ **ç¸½è¨ˆçµ±è¨ˆ**")
                    result.append(f"   ç¸½ Collections: {summary.get('total_collections', 0)}")
                    result.append(f"   ç¸½ Points: {summary.get('total_points', 0):,}")
                    result.append(f"   ç¸½ Vectors: {summary.get('total_vectors', 0):,}")
                    result.append(f"   ä¼°è¨ˆè¨˜æ†¶é«”: {summary.get('total_estimated_memory_mb', 0):.1f}MB")
                    result.append("")
                
                # æ€§èƒ½åˆ†æ
                performance = report.get('performance_analysis', {})
                if performance and 'error' not in performance:
                    result.append("âš¡ **æ•ˆèƒ½åˆ†æ**")
                    score = performance.get('overall_score', 'unknown')
                    score_emoji = {"excellent": "ğŸŸ¢", "good": "ğŸŸ¡", "fair": "ğŸŸ ", "poor": "ğŸ”´"}.get(score, "âšª")
                    result.append(f"   æ•´é«”è©•åˆ†: {score_emoji} {score}")
                    
                    recommendations = performance.get('recommendations', [])
                    if recommendations:
                        result.append("   ğŸ’¡ å»ºè­°:")
                        for rec in recommendations:
                            result.append(f"      â€¢ {rec}")
                    
                    indexing_issues = performance.get('indexing_issues', [])
                    if indexing_issues:
                        result.append("   âš ï¸ ç´¢å¼•å•é¡Œ:")
                        for issue in indexing_issues:
                            result.append(f"      â€¢ {issue['collection']}: {issue['issue']} ({issue['indexed_ratio']:.1%})")
                    result.append("")
                
                # é›†ç¾¤è³‡è¨Š
                cluster = report.get('cluster_info', {})
                if cluster and 'error' not in cluster:
                    result.append("ğŸŒ **é›†ç¾¤è³‡è¨Š**")
                    if cluster.get('status') == 'healthy':
                        result.append("   ç‹€æ…‹: âœ… å¥åº·")
                    result.append("")
                
                # ç›£æ§ç¯„åœèªªæ˜
                result.append("â„¹ï¸ **ç›£æ§ç¯„åœ**")
                result.append(f"   ç›£æ§é¡å‹: {report.get('monitoring_scope', 'unknown')}")
                limitations = report.get('limitations', [])
                if limitations:
                    result.append("   é™åˆ¶:")
                    for limitation in limitations:
                        result.append(f"      â€¢ {limitation}")
                
                return result
                
            except Exception as e:
                logger.error(f"ç²å– Qdrant ç‹€æ…‹å¤±æ•—: {e}")
                return [
                    "âŒ **Qdrant ç‹€æ…‹åˆ†æå¤±æ•—**",
                    f"éŒ¯èª¤: {str(e)}",
                    "",
                    "é€™å¯èƒ½æ˜¯å› ç‚º:",
                    "â€¢ Qdrant æœå‹™æœªé‹è¡Œ",
                    "â€¢ é€£æ¥é…ç½®éŒ¯èª¤", 
                    "â€¢ ç¶²è·¯å•é¡Œ"
                ]

        async def get_qdrant_performance(ctx: Context) -> list[str]:
            """
            ç²å– Qdrant çš„æ•ˆèƒ½åˆ†æï¼Œåƒ…åŸºæ–¼ Qdrant API è³‡æ–™ã€‚
            """
            await ctx.debug("Getting Qdrant performance analysis")
            try:
                collections_info = await self.system_monitor.get_collections_info()
                performance = self.system_monitor._analyze_performance(collections_info)
                
                result = ["ğŸ“Š **Qdrant æ•ˆèƒ½åˆ†æ**"]
                result.append("")
                
                if 'error' in performance:
                    result.append(f"âŒ åˆ†æå¤±æ•—: {performance['error']}")
                    return result
                    
                # æ•´é«”æ•ˆèƒ½è©•åˆ†
                score = performance.get('overall_score', 'unknown')
                score_emoji = {"excellent": "ğŸŸ¢", "good": "ğŸŸ¡", "fair": "ğŸŸ ", "poor": "ğŸ”´"}.get(score, "âšª")
                result.append(f"âš¡ **æ•´é«”æ•ˆèƒ½**: {score_emoji} {score}")
                result.append("")
                
                # è¨˜æ†¶é«”åˆ†æ
                total_memory = performance.get('total_estimated_memory_mb', 0)
                total_vectors = performance.get('total_vectors', 0)
                result.append("ğŸ’¾ **è¨˜æ†¶é«”åˆ†æ**")
                result.append(f"   ä¼°è¨ˆç¸½è¨˜æ†¶é«”: {total_memory:.1f} MB")
                result.append(f"   ç¸½å‘é‡æ•¸: {total_vectors:,}")
                if total_vectors > 0:
                    avg_memory_per_vector = (total_memory * 1024) / total_vectors  # KB per vector
                    result.append(f"   å¹³å‡æ¯å‘é‡: {avg_memory_per_vector:.2f} KB")
                result.append("")
                
                # Collection æ•ˆèƒ½åˆ†æ
                collection_analysis = performance.get('collection_analysis', [])
                if collection_analysis:
                    result.append("ï¿½ **Collection æ•ˆèƒ½**")
                    for analysis in collection_analysis:
                        name = analysis.get('name', 'unknown')
                        efficiency = analysis.get('efficiency', 'unknown')
                        indexed_ratio = analysis.get('indexed_ratio', 0)
                        
                        eff_emoji = {"good": "ğŸŸ¢", "fair": "ğŸŸ¡", "poor": "ğŸ”´"}.get(efficiency, "âšª")
                        result.append(f"   ğŸ“‚ {name}:")
                        result.append(f"      æ•ˆç‡: {eff_emoji} {efficiency}")
                        result.append(f"      ç´¢å¼•ç‡: {indexed_ratio:.1%}")
                result.append("")
                
                # å»ºè­°
                recommendations = performance.get('recommendations', [])
                if recommendations:
                    result.append("ğŸ’¡ **æœ€ä½³åŒ–å»ºè­°**")
                    for i, rec in enumerate(recommendations, 1):
                        result.append(f"   {i}. {rec}")
                    result.append("")
                
                # ç´¢å¼•å•é¡Œ
                indexing_issues = performance.get('indexing_issues', [])
                if indexing_issues:
                    result.append("âš ï¸ **ç´¢å¼•å•é¡Œ**")
                    for issue in indexing_issues:
                        result.append(f"   â€¢ {issue['collection']}: {issue['issue']} (ç´¢å¼•ç‡: {issue['indexed_ratio']:.1%})")
                    result.append("")
                
                result.append("â„¹ï¸ **èªªæ˜**: æ­¤åˆ†æåƒ…åŸºæ–¼ Qdrant API æä¾›çš„è³‡è¨Š")
                
                return result
                
            except Exception as e:
                logger.error(f"ç²å–æ•ˆèƒ½åˆ†æå¤±æ•—: {e}")
                return [
                    "âŒ **æ•ˆèƒ½åˆ†æå¤±æ•—**",
                    f"éŒ¯èª¤: {str(e)}"
                ]

        async def get_collections_detailed_analysis(ctx: Context) -> list[str]:
            """
            ç²å– Collections çš„è©³ç´°åˆ†æï¼ŒåŒ…æ‹¬æ•ˆèƒ½è©•ä¼°å’Œæœ€ä½³åŒ–å»ºè­°ã€‚
            """
            await ctx.debug("Getting detailed collections analysis")
            try:
                collections_info = await self.system_monitor.get_collections_info()
                
                if not collections_info or 'collections' not in collections_info:
                    return ["ğŸ“‹ æ²’æœ‰ç™¼ç¾ä»»ä½• Collections"]
                
                result = ["ğŸ“Š **Collections è©³ç´°åˆ†æ**"]
                result.append("")
                
                for collection in collections_info['collections']:
                    if isinstance(collection, dict) and 'name' in collection:
                        if 'error' in collection:
                            result.append(f"âŒ **{collection['name']}** - åˆ†æå¤±æ•—: {collection['error']}")
                            continue
                        
                        result.append(f"ğŸ“ **Collection: {collection['name']}**")
                        
                        # åŸºæœ¬çµ±è¨ˆ
                        result.append("   ğŸ“ˆ çµ±è¨ˆè³‡è¨Š:")
                        result.append(f"      Points: {collection.get('points_count', 0):,}")
                        result.append(f"      Vectors: {collection.get('vectors_count', 0):,}")
                        result.append(f"      å·²ç´¢å¼•: {collection.get('indexed_vectors_count', 0):,}")
                        
                        # è¨ˆç®—ç´¢å¼•ç‡
                        vectors_count = collection.get('vectors_count', 0)
                        indexed_count = collection.get('indexed_vectors_count', 0)
                        indexing_ratio = indexed_count / vectors_count if vectors_count > 0 else 0
                        result.append(f"      ç´¢å¼•ç‡: {indexing_ratio:.1%}")
                        
                        # å‘é‡é…ç½®
                        vector_size = collection.get('vector_size', 0)
                        if vector_size > 0:
                            result.append("   ğŸ”§ å‘é‡é…ç½®:")
                            result.append(f"      å‘é‡ç¶­åº¦: {vector_size} ç¶­")
                        
                        # è¨˜æ†¶é«”åˆ†æ
                        memory_mb = collection.get('estimated_memory_mb', 0)
                        result.append("   ğŸ§  è¨˜æ†¶é«”åˆ†æ:")
                        result.append(f"      ç¸½ä¼°ç®—: {memory_mb:.2f} MB")
                        if collection.get('points_count', 0) > 0:
                            memory_per_point = memory_mb / collection['points_count']
                            result.append(f"      æ¯é»è¨˜æ†¶é«”: {memory_per_point:.4f} MB")
                        
                        # ç‹€æ…‹è³‡è¨Š
                        status = collection.get('status', 'unknown')
                        result.append("   âš™ï¸ ç‹€æ…‹:")
                        result.append(f"      Collection: {status}")
                        
                        result.append("")
                
                return result
                
            except Exception as e:
                logger.error(f"Failed to get collections analysis: {e}")
                return [f"âŒ ç²å– Collections åˆ†æå¤±æ•—: {str(e)}"]

        async def get_deployment_info(ctx: Context) -> list[str]:
            """
            ç²å–éƒ¨ç½²ç’°å¢ƒè³‡è¨Šå’Œæ”¯æ´çš„åŠŸèƒ½ã€‚
            """
            await ctx.debug("Getting deployment information")
            try:
                deployment_info = self.system_monitor.deployment_info
                
                result = ["ğŸ—ï¸ **éƒ¨ç½²ç’°å¢ƒè³‡è¨Š**"]
                result.append("")
                
                result.append(f"ï¿½ **åŸºæœ¬è³‡è¨Š**")
                result.append(f"   é¡å‹: {deployment_info.get('type', 'unknown')}")
                result.append(f"   ä¸»æ©Ÿ: {deployment_info.get('host', 'unknown')}")
                result.append(f"   ç«¯å£: {deployment_info.get('port', 'unknown')}")
                result.append("")
                
                result.append(f"ğŸ”§ **ç’°å¢ƒç‰¹æ€§**")
                result.append(f"   æœ¬åœ°éƒ¨ç½²: {'âœ…' if deployment_info.get('is_local') else 'âŒ'}")
                result.append(f"   é›²ç«¯æœå‹™: {'âœ…' if deployment_info.get('is_cloud') else 'âŒ'}")
                result.append(f"   Docker å®¹å™¨: {'âœ…' if deployment_info.get('is_docker') else 'âŒ'}")
                result.append("")
                
                # å¯ç”¨åŠŸèƒ½
                features = deployment_info.get('features', [])
                result.append(f"âœ¨ **å¯ç”¨åŠŸèƒ½** ({len(features)} é …)")
                feature_descriptions = {
                    'qdrant_api': 'ğŸ”Œ Qdrant API æŸ¥è©¢',
                    'health_check': 'ğŸ¥ å¥åº·ç‹€æ…‹æª¢æŸ¥',
                    'collections_stats': 'ğŸ“Š Collections çµ±è¨ˆ',
                    'system_metrics': 'ğŸ’» ç³»çµ±è³‡æºç›£æ§',
                    'docker_stats': 'ğŸ³ Docker å®¹å™¨ç›£æ§',
                    'container_logs': 'ğŸ“‹ å®¹å™¨æ—¥èªŒæŸ¥çœ‹'
                }
                
                for feature in features:
                    desc = feature_descriptions.get(feature, f'ğŸ”§ {feature}')
                    result.append(f"   {desc}")
                result.append("")
                
                # é™åˆ¶
                limitations = deployment_info.get('limitations', [])
                if limitations:
                    result.append(f"âš ï¸ **åŠŸèƒ½é™åˆ¶** ({len(limitations)} é …)")
                    limitation_descriptions = {
                        'no_system_metrics': 'ğŸ’» ç„¡æ³•ç²å–ç³»çµ±è³‡æºè³‡è¨Š',
                        'no_container_access': 'ğŸ³ ç„¡æ³•å­˜å–å®¹å™¨è³‡è¨Š',
                        'no_logs_access': 'ğŸ“‹ ç„¡æ³•ç²å–æœå‹™æ—¥èªŒ',
                        'no_docker_stats': 'ğŸ“Š ç„¡æ³•ç²å– Docker çµ±è¨ˆ'
                    }
                    
                    for limitation in limitations:
                        desc = limitation_descriptions.get(limitation, f'âš ï¸ {limitation}')
                        result.append(f"   {desc}")
                    result.append("")
                
                # éƒ¨ç½²å»ºè­°
                deployment_type = deployment_info.get('type', 'unknown')
                result.append("ğŸ’¡ **éƒ¨ç½²å»ºè­°**")
                
                if deployment_type == 'cloud':
                    result.append("   â€¢ é›²ç«¯éƒ¨ç½²ï¼šé—œæ³¨æˆæœ¬æœ€ä½³åŒ–å’Œè³‡æ–™å‚³è¼¸")
                    result.append("   â€¢ å»ºè­°è¨­å®šé©ç•¶çš„æŸ¥è©¢é™åˆ¶å’Œå¿«å–ç­–ç•¥")
                    result.append("   â€¢ å®šæœŸæª¢æŸ¥ API ä½¿ç”¨é‡å’Œè¨ˆè²»")
                elif deployment_type == 'docker_local':
                    result.append("   â€¢ Docker éƒ¨ç½²ï¼šç›£æ§å®¹å™¨è³‡æºä½¿ç”¨")
                    result.append("   â€¢ å»ºè­°è¨­å®šè¨˜æ†¶é«”é™åˆ¶å’Œå¥åº·æª¢æŸ¥")
                    result.append("   â€¢ å®šæœŸå‚™ä»½è³‡æ–™å’Œé…ç½®")
                elif deployment_type == 'local_binary':
                    result.append("   â€¢ æœ¬åœ°éƒ¨ç½²ï¼šæ³¨æ„ç³»çµ±è³‡æºç®¡ç†")
                    result.append("   â€¢ å»ºè­°è¨­å®šé©ç•¶çš„ç³»çµ±ç›£æ§")
                    result.append("   â€¢ ç¢ºä¿è³‡æ–™å‚™ä»½å’Œæœå‹™è‡ªå‹•é‡å•Ÿ")
                else:
                    result.append("   â€¢ å»ºè­°æ ¹æ“šå¯¦éš›éœ€æ±‚é¸æ“‡åˆé©çš„ç›£æ§ç­–ç•¥")
                
                return result
                
            except Exception as e:
                logger.error(f"Failed to get deployment info: {e}")
                return [f"âŒ ç²å–éƒ¨ç½²è³‡è¨Šå¤±æ•—: {str(e)}"]

        async def get_docker_containers(ctx: Context) -> list[str]:
            """
            ç²å– Qdrant ç›¸é—œçš„ Docker å®¹å™¨è³‡è¨Šã€‚
            """
            await ctx.debug("Getting Docker containers info")
            try:
                from .system_monitor_backup import DockerSystemMonitor
                containers = DockerSystemMonitor.list_qdrant_containers()
                
                if not containers:
                    return ["ğŸ“¦ æ²’æœ‰ç™¼ç¾ Qdrant ç›¸é—œçš„ Docker å®¹å™¨"]
                
                result = ["ğŸ³ **Qdrant Docker å®¹å™¨**"]
                result.append("")
                
                for container in containers:
                    result.append(f"ğŸ“¦ **å®¹å™¨: {container.get('name', 'unknown')}**")
                    result.append(f"   ç‹€æ…‹: {container.get('status', 'unknown')}")
                    result.append(f"   æ˜ åƒ: {container.get('image', 'unknown')}")
                    result.append(f"   ç«¯å£: {container.get('ports', 'none')}")
                    result.append("")
                
                return result
                
            except Exception as e:
                logger.error(f"Failed to get Docker containers: {e}")
                return [f"âŒ ç²å– Docker å®¹å™¨è³‡è¨Šå¤±æ•—: {str(e)}"]

        async def get_container_logs(
            ctx: Context,
            container_name: Annotated[str, Field(description="å®¹å™¨åç¨±")] = "qdrant",
            lines: Annotated[int, Field(description="æ—¥èªŒè¡Œæ•¸")] = 50
        ) -> list[str]:
            """
            ç²å–æŒ‡å®šå®¹å™¨çš„æ—¥èªŒã€‚
            """
            await ctx.debug(f"Getting container logs for {container_name}")
            try:
                from .system_monitor_backup import DockerSystemMonitor
                logs = DockerSystemMonitor.get_container_logs(container_name, lines)
                
                result = [f"ğŸ“‹ **å®¹å™¨æ—¥èªŒ: {container_name} (æœ€è¿‘ {lines} è¡Œ)**"]
                result.append("```")
                result.append(logs)
                result.append("```")
                
                return result
                
            except Exception as e:
                logger.error(f"Failed to get container logs: {e}")
                return [f"âŒ ç²å–å®¹å™¨æ—¥èªŒå¤±æ•—: {str(e)}"]

        # è¨»å†Šé€šç”¨ç³»çµ±ç›£æ§å·¥å…·
        async def get_qdrant_status(ctx: Context) -> list[str]:
            """
            ç²å– Qdrant çš„ç¶œåˆç‹€æ…‹è³‡è¨Šï¼Œè‡ªå‹•é©æ‡‰æ‰€æœ‰éƒ¨ç½²æ–¹å¼ã€‚
            æ”¯æ´ Cloudã€Dockerã€Localã€Remote ç­‰å¤šç¨®éƒ¨ç½²ç’°å¢ƒã€‚
            """
            await ctx.debug("Getting comprehensive Qdrant status")
            
            try:
                status = await self.system_monitor.get_comprehensive_analysis()
                
                result = ["ğŸ” **Qdrant ç¶œåˆç‹€æ…‹åˆ†æ**"]
                result.append("")
                
                # éƒ¨ç½²è³‡è¨Š
                deployment = status.get("deployment_info", {})
                result.append(f"ğŸ“Š **éƒ¨ç½²é¡å‹**: {deployment.get('type', 'unknown')}")
                result.append(f"ğŸŒ **ç«¯é»**: {deployment.get('url', 'unknown')}")
                result.append(f"ğŸ  **ä¸»æ©Ÿ**: {deployment.get('host', 'unknown')}:{deployment.get('port', 'unknown')}")
                result.append("")
                
                # å¯ç”¨åŠŸèƒ½
                features = deployment.get("features", [])
                result.append("âœ… **å¯ç”¨åŠŸèƒ½**:")
                for feature in features:
                    result.append(f"   â€¢ {feature}")
                result.append("")
                
                # é™åˆ¶èªªæ˜
                limitations = deployment.get("limitations", [])
                if limitations:
                    result.append("âš ï¸ **åŠŸèƒ½é™åˆ¶**:")
                    for limitation in limitations:
                        result.append(f"   â€¢ {limitation}")
                    result.append("")
                
                # Qdrant å¥åº·ç‹€æ…‹
                health = status.get("qdrant_health", {})
                result.append(f"ğŸ’š **Qdrant ç‹€æ…‹**: {health.get('status', 'unknown')}")
                result.append(f"ğŸ“ **Collections æ•¸é‡**: {health.get('collections_count', 0)}")
                
                # æ¨£æœ¬ Collection ç‹€æ…‹
                if "sample_collection_status" in health:
                    sample = health["sample_collection_status"]
                    result.append(f"ğŸ“‚ **æ¨£æœ¬ Collection**: {sample.get('name', 'unknown')}")
                    result.append(f"   â€¢ æ–‡ä»¶æ•¸é‡: {sample.get('points_count', 0)}")
                    result.append(f"   â€¢ ç‹€æ…‹: {sample.get('status', 'unknown')}")
                result.append("")
                
                # ç³»çµ±æŒ‡æ¨™ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                system_metrics = status.get("system_metrics", {})
                if not system_metrics.get("unavailable"):
                    result.append("ğŸ’» **ç³»çµ±è³‡æº**:")
                    if "cpu" in system_metrics:
                        cpu = system_metrics["cpu"]
                        result.append(f"   â€¢ CPU: {cpu.get('usage_percent', 0)}% ({cpu.get('core_count', 0)} æ ¸å¿ƒ)")
                    if "memory" in system_metrics:
                        memory = system_metrics["memory"]
                        result.append(f"   â€¢ è¨˜æ†¶é«”: {memory.get('usage_percent', 0)}% ({memory.get('used_gb', 0):.1f}GB / {memory.get('total_gb', 0):.1f}GB)")
                    if "disk" in system_metrics:
                        disk = system_metrics["disk"]
                        result.append(f"   â€¢ ç£ç¢Ÿ: {disk.get('usage_percent', 0)}% ({disk.get('used_gb', 0):.1f}GB / {disk.get('total_gb', 0):.1f}GB)")
                else:
                    result.append("ğŸ’» **ç³»çµ±è³‡æº**: ä¸å¯ç”¨")
                    result.append(f"   åŸå› : {system_metrics.get('reason', 'unknown')}")
                    if "alternatives" in system_metrics:
                        result.append("   ğŸ’¡ **æ›¿ä»£æ–¹æ¡ˆ**:")
                        for alt in system_metrics["alternatives"]:
                            result.append(f"      â€¢ {alt}")
                result.append("")
                
                # Docker æŒ‡æ¨™ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                docker_metrics = status.get("docker_metrics", {})
                if not docker_metrics.get("unavailable"):
                    result.append("ğŸ³ **Docker å®¹å™¨**:")
                    result.append(f"   â€¢ å®¹å™¨: {docker_metrics.get('container_name', 'unknown')}")
                    result.append(f"   â€¢ CPU: {docker_metrics.get('cpu_percent', 'unknown')}")
                    result.append(f"   â€¢ è¨˜æ†¶é«”: {docker_metrics.get('memory_usage', 'unknown')} ({docker_metrics.get('memory_percent', 'unknown')})")
                    result.append(f"   â€¢ ç¶²è·¯ I/O: {docker_metrics.get('network_io', 'unknown')}")
                else:
                    result.append("ğŸ³ **Docker æŒ‡æ¨™**: ä¸å¯ç”¨")
                    result.append(f"   åŸå› : {docker_metrics.get('reason', 'unknown')}")
                result.append("")
                
                # é›²ç«¯è³‡è¨Šï¼ˆå¦‚æœæ˜¯é›²ç«¯éƒ¨ç½²ï¼‰
                if "cloud_info" in status:
                    cloud = status["cloud_info"]
                    result.append("â˜ï¸ **é›²ç«¯æœå‹™è³‡è¨Š**:")
                    result.append(f"   â€¢ æä¾›å•†: {cloud.get('provider', 'unknown')}")
                    if "dashboard_url" in cloud:
                        result.append(f"   â€¢ æ§åˆ¶å°: {cloud['dashboard_url']}")
                    if "features" in cloud:
                        result.append("   â€¢ é›²ç«¯åŠŸèƒ½:")
                        for feature in cloud["features"]:
                            result.append(f"      â€¢ {feature}")
                    result.append("")
                
                # é ç«¯éƒ¨ç½²å»ºè­°ï¼ˆå¦‚æœæ˜¯é ç«¯éƒ¨ç½²ï¼‰
                if "remote_monitoring_suggestions" in status:
                    result.append("ğŸ”— **é ç«¯ç›£æ§å»ºè­°**:")
                    for suggestion in status["remote_monitoring_suggestions"]:
                        result.append(f"   â€¢ {suggestion}")
                    result.append("")
                
                result.append(f"ğŸ• **æ›´æ–°æ™‚é–“**: {status.get('timestamp', 'unknown')}")
                
                return result
                
            except Exception as e:
                logger.error(f"Failed to get Qdrant status: {e}")
                return [f"âŒ ç²å–ç‹€æ…‹å¤±æ•—: {str(e)}"]

        async def get_collections_detailed_analysis(ctx: Context) -> list[str]:
            """
            ç²å– Collections çš„è©³ç´°æ•ˆèƒ½åˆ†æï¼ŒåŒ…æ‹¬éƒ¨ç½²ç‰¹å®šçš„æœ€ä½³åŒ–å»ºè­°ã€‚
            """
            await ctx.debug("Getting detailed collections analysis")
            
            try:
                analytics = await self.system_monitor.get_collections_info()
                
                result = ["ğŸ“Š **Collections è©³ç´°åˆ†æ**"]
                result.append("")
                
                if not analytics or 'collections' not in analytics:
                    result.append("âŒ ç„¡æ³•ç²å– Collections è³‡è¨Š")
                    return result
                
                for collection in analytics['collections']:
                    if "error" in collection:
                        result.append(f"âŒ **{collection.get('name', 'unknown')}**: {collection['error']}")
                        continue
                    
                    result.append(f"ğŸ“ **Collection: {collection['name']}**")
                    
                    # åŸºæœ¬çµ±è¨ˆ
                    result.append(f"   ğŸ“ˆ **çµ±è¨ˆè³‡æ–™**:")
                    result.append(f"      â€¢ æ–‡ä»¶æ•¸: {collection.get('points_count', 0):,}")
                    result.append(f"      â€¢ å‘é‡æ•¸: {collection.get('vectors_count', 0):,}")
                    result.append(f"      â€¢ å·²ç´¢å¼•å‘é‡: {collection.get('indexed_vectors_count', 0):,}")
                    
                    # è¨ˆç®—ç´¢å¼•ç‡
                    vectors_count = collection.get('vectors_count', 0)
                    indexed_count = collection.get('indexed_vectors_count', 0)
                    indexing_ratio = indexed_count / vectors_count if vectors_count > 0 else 0
                    result.append(f"      â€¢ ç´¢å¼•ç‡: {indexing_ratio:.1%}")
                    
                    # è¨˜æ†¶é«”ä¼°ç®—
                    memory_mb = collection.get('estimated_memory_mb', 0)
                    result.append(f"      â€¢ è¨˜æ†¶é«”ä¼°ç®—: {memory_mb:.1f}MB")
                    
                    # å‘é‡é…ç½®
                    vector_size = collection.get('vector_size', 0)
                    if vector_size > 0:
                        result.append(f"   ğŸ¯ **å‘é‡é…ç½®**:")
                        result.append(f"      â€¢ å‘é‡ç¶­åº¦: {vector_size}")
                    
                    # ç‹€æ…‹è³‡è¨Š
                    status = collection.get('status', 'unknown')
                    result.append(f"   ğŸ’š **ç‹€æ…‹**: {status}")
                    result.append("")
                    
                    # è¨˜æ†¶é«”åˆ†æ
                    memory = collection.get("memory_analysis", {})
                    result.append(f"   ğŸ’¾ **è¨˜æ†¶é«”åˆ†æ**:")
                    result.append(f"      â€¢ ç¸½è¨ˆ: {memory.get('total_estimate_mb', 0):.1f}MB")
                    result.append(f"      â€¢ æ¯æ–‡ä»¶: {memory.get('memory_per_point_mb', 0):.3f}MB")
                    result.append(f"      â€¢ æ•ˆèƒ½è©•åˆ†: {memory.get('performance_score', 'unknown')}")
                    
                    # å»ºè­°
                    recommendations = collection.get("recommendations", [])
                    if recommendations:
                        result.append(f"   ğŸ’¡ **æœ€ä½³åŒ–å»ºè­°**:")
                        for rec in recommendations:
                            result.append(f"      â€¢ {rec}")
                    
                    # éƒ¨ç½²ç‰¹å®šæœ€ä½³åŒ–
                    deploy_opts = collection.get("deployment_optimizations", [])
                    if deploy_opts:
                        result.append(f"   ğŸš€ **éƒ¨ç½²æœ€ä½³åŒ–**:")
                        for opt in deploy_opts:
                            result.append(f"      â€¢ {opt}")
                    
                    result.append("")
                
                return result
                
            except Exception as e:
                logger.error(f"Failed to get collections analysis: {e}")
                return [f"âŒ ç²å–åˆ†æå¤±æ•—: {str(e)}"]

        async def get_deployment_info(ctx: Context) -> list[str]:
            """
            ç²å–è©³ç´°çš„éƒ¨ç½²ç’°å¢ƒè³‡è¨Šå’ŒåŠŸèƒ½æ”¯æ´ç‹€æ³ã€‚
            """
            await ctx.debug("Getting deployment information")
            
            try:
                deployment = self.system_monitor.deployment_info
                
                result = ["ğŸ—ï¸ **éƒ¨ç½²ç’°å¢ƒè©³ç´°è³‡è¨Š**"]
                result.append("")
                
                result.append(f"ğŸ“ **åŸºæœ¬è³‡è¨Š**:")
                result.append(f"   â€¢ éƒ¨ç½²é¡å‹: {deployment.get('type', 'unknown')}")
                result.append(f"   â€¢ ä¸»æ©Ÿ: {deployment.get('host', 'unknown')}")
                result.append(f"   â€¢ ç«¯å£: {deployment.get('port', 'unknown')}")
                result.append(f"   â€¢ URL: {deployment.get('url', 'unknown')}")
                result.append("")
                
                result.append(f"ğŸ“Š **éƒ¨ç½²ç‰¹æ€§**:")
                result.append(f"   â€¢ æœ¬åœ°éƒ¨ç½²: {'âœ…' if deployment.get('is_local') else 'âŒ'}")
                result.append(f"   â€¢ é›²ç«¯éƒ¨ç½²: {'âœ…' if deployment.get('is_cloud') else 'âŒ'}")
                result.append(f"   â€¢ Docker éƒ¨ç½²: {'âœ…' if deployment.get('is_docker') else 'âŒ'}")
                result.append(f"   â€¢ é ç«¯éƒ¨ç½²: {'âœ…' if deployment.get('is_remote') else 'âŒ'}")
                result.append("")
                
                features = deployment.get("features", [])
                result.append("âœ… **æ”¯æ´åŠŸèƒ½**:")
                if features:
                    for feature in features:
                        feature_name = {
                            "qdrant_api": "Qdrant API å­˜å–",
                            "health_check": "å¥åº·æª¢æŸ¥",
                            "collections_stats": "Collections çµ±è¨ˆ",
                            "system_metrics": "ç³»çµ±è³‡æºç›£æ§",
                            "docker_stats": "Docker å®¹å™¨ç›£æ§",
                            "container_logs": "å®¹å™¨æ—¥èªŒå­˜å–",
                            "container_metrics": "å®¹å™¨æŒ‡æ¨™",
                            "cloud_monitoring": "é›²ç«¯ç›£æ§åŠŸèƒ½"
                        }.get(feature, feature)
                        result.append(f"   â€¢ {feature_name}")
                else:
                    result.append("   â€¢ ç„¡ç‰¹æ®ŠåŠŸèƒ½")
                result.append("")
                
                limitations = deployment.get("limitations", [])
                if limitations:
                    result.append("âš ï¸ **åŠŸèƒ½é™åˆ¶**:")
                    for limitation in limitations:
                        limitation_name = {
                            "no_system_metrics": "ç„¡æ³•å–å¾—ç³»çµ±è³‡æºæŒ‡æ¨™",
                            "no_container_access": "ç„¡æ³•å­˜å–å®¹å™¨è³‡è¨Š",
                            "no_logs_access": "ç„¡æ³•å­˜å–æ—¥èªŒæª”æ¡ˆ",
                            "no_container_logs": "ç„¡æ³•å­˜å–å®¹å™¨æ—¥èªŒ"
                        }.get(limitation, limitation)
                        result.append(f"   â€¢ {limitation_name}")
                    result.append("")
                
                # ç›£æ§å»ºè­°
                result.append("ğŸ’¡ **ç›£æ§å»ºè­°**:")
                if deployment.get('is_cloud'):
                    result.append("   â€¢ ä½¿ç”¨é›²ç«¯æœå‹™å•†çš„ç›£æ§å·¥å…·")
                    result.append("   â€¢ è¨­å®š API ä½¿ç”¨é‡å‘Šè­¦")
                    result.append("   â€¢ ç›£æ§æˆæœ¬å’Œé…é¡")
                elif deployment.get('is_docker'):
                    result.append("   â€¢ ä½¿ç”¨ Docker å¥åº·æª¢æŸ¥")
                    result.append("   â€¢ ç›£æ§å®¹å™¨è³‡æºä½¿ç”¨")
                    result.append("   â€¢ è¨­å®šé©ç•¶çš„é‡å•Ÿç­–ç•¥")
                elif deployment.get('is_local'):
                    result.append("   â€¢ ç›£æ§æœ¬åœ°ç³»çµ±è³‡æº")
                    result.append("   â€¢ è¨­å®šæœå‹™ç®¡ç†å’Œè‡ªå‹•å•Ÿå‹•")
                    result.append("   â€¢ å®šæœŸå‚™ä»½è³‡æ–™")
                else:
                    result.append("   â€¢ ç›£æ§ç¶²è·¯é€£ç·šå“è³ª")
                    result.append("   â€¢ è¨­å®šå¥åº·æª¢æŸ¥ç«¯é»")
                    result.append("   â€¢ ä½¿ç”¨å¤–éƒ¨ç›£æ§æœå‹™")
                
                return result
                
            except Exception as e:
                logger.error(f"Failed to get deployment info: {e}")
                return [f"âŒ ç²å–éƒ¨ç½²è³‡è¨Šå¤±æ•—: {str(e)}"]

        # è¨»å†Šç´” Qdrant API ç›£æ§å·¥å…·
        self.tool(
            get_qdrant_status,
            name="qdrant-system-status",
            description="ç²å– Qdrant çš„ç‹€æ…‹è³‡è¨Šï¼Œç´”åŸºæ–¼ Qdrant APIï¼Œé©ç”¨æ‰€æœ‰éƒ¨ç½²æ–¹å¼ã€‚",
        )
        
        self.tool(
            get_qdrant_performance,
            name="qdrant-performance-analysis",
            description="ç²å– Qdrant çš„æ•ˆèƒ½åˆ†æï¼ŒåŸºæ–¼ Qdrant API è³‡æ–™æä¾›è¨˜æ†¶é«”ä½¿ç”¨å’Œæ•ˆèƒ½è©•ä¼°ã€‚",
        )

        # æ–°å¢å„²å­˜å„ªåŒ–å·¥å…·
        async def optimize_storage(
            ctx: Context,
            collection_name: Annotated[
                str, Field(description="è¦å„ªåŒ–çš„ collection åç¨±ï¼Œè¨­ç‚º 'all' è¡¨ç¤ºå„ªåŒ–æ‰€æœ‰ collections")
            ] = "all",
            confirm_optimize: Annotated[
                bool, Field(description="ç¢ºèªå„ªåŒ–æ“ä½œï¼Œå¿…é ˆè¨­ç‚º True")
            ] = False,
        ) -> list[str]:
            """
            å„ªåŒ–æŒ‡å®š collection æˆ–æ‰€æœ‰ collections çš„å„²å­˜é…ç½®ï¼Œæ¸›å°‘ç£ç¢Ÿä½¿ç”¨ã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚
            """
            await ctx.debug(f"Optimizing storage for collection '{collection_name}'")
            
            if not confirm_optimize:
                return [
                    "âŒ **å„²å­˜å„ªåŒ–è¢«å–æ¶ˆ**",
                    "",
                    "è«‹è¨­å®š confirm_optimize=True ä¾†ç¢ºèªåŸ·è¡Œå„²å­˜å„ªåŒ–ã€‚",
                    "",
                    "âš ï¸ **æ³¨æ„**: å„ªåŒ–å¾Œå»ºè­°é‡å•Ÿ Qdrant container ä»¥å®Œå…¨æ‡‰ç”¨è®Šæ›´ã€‚"
                ]
            
            try:
                if collection_name == "all":
                    result = await self.storage_optimizer.optimize_all_collections()
                else:
                    result = await self.storage_optimizer.optimize_collection_storage(collection_name)
                
                if not result.get("success", False):
                    return [
                        f"âŒ **å„²å­˜å„ªåŒ–å¤±æ•—**",
                        f"éŒ¯èª¤: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}"
                    ]
                
                # æ ¼å¼åŒ–æˆåŠŸçµæœ
                formatted_result = ["âœ… **Qdrant å„²å­˜å„ªåŒ–å®Œæˆ**"]
                formatted_result.append("")
                
                if collection_name == "all":
                    # æ‰¹æ¬¡å„ªåŒ–çµæœ
                    summary = result.get("summary", {})
                    formatted_result.append("ğŸ“Š **å„ªåŒ–ç¸½çµ**")
                    formatted_result.append(f"   Collections å„ªåŒ–: {result.get('collections_optimized', 0)}/{result.get('total_collections', 0)}")
                    formatted_result.append(f"   ç£ç¢Ÿç©ºé–“: {summary.get('disk_space_before_mb', 0):.1f}MB â†’ {summary.get('disk_space_after_mb', 0):.1f}MB")
                    formatted_result.append(f"   ç¯€çœç£ç¢Ÿ: {summary.get('disk_space_saved_mb', 0):.1f}MB ({summary.get('disk_space_saved_percent', 0):.1f}%)")
                    formatted_result.append(f"   è¨˜æ†¶é«”: {summary.get('ram_before_mb', 0):.1f}MB â†’ {summary.get('ram_after_mb', 0):.1f}MB")
                    formatted_result.append(f"   ç¯€çœè¨˜æ†¶é«”: {summary.get('ram_saved_mb', 0):.1f}MB ({summary.get('ram_saved_percent', 0):.1f}%)")
                    formatted_result.append("")
                    
                    # å€‹åˆ¥ collection çµæœ
                    if result.get("results"):
                        formatted_result.append("ğŸ“ **å„ Collection å„ªåŒ–çµæœ**")
                        for coll_result in result["results"]:
                            if coll_result.get("success"):
                                name = coll_result["collection"]
                                before_disk = coll_result["before"].get("disk_size", 0) / 1024 / 1024
                                after_disk = coll_result["after"].get("disk_size", 0) / 1024 / 1024
                                formatted_result.append(f"   âœ… {name}: {before_disk:.1f}MB â†’ {after_disk:.1f}MB")
                            else:
                                formatted_result.append(f"   âŒ {coll_result['collection']}: {coll_result.get('error', 'å„ªåŒ–å¤±æ•—')}")
                        formatted_result.append("")
                    
                    # å»ºè­°
                    recommendations = result.get("recommendations", [])
                    if recommendations:
                        formatted_result.append("ğŸ’¡ **å»ºè­°**")
                        for rec in recommendations:
                            formatted_result.append(f"   â€¢ {rec}")
                        formatted_result.append("")
                
                else:
                    # å–®ä¸€ collection å„ªåŒ–çµæœ
                    before = result.get("before", {})
                    after = result.get("after", {})
                    optimizations = result.get("optimizations_applied", {})
                    
                    formatted_result.append(f"ğŸ“ **Collection: {collection_name}**")
                    formatted_result.append(f"   å‘é‡æ•¸: {before.get('vectors_count', 0):,} â†’ {after.get('vectors_count', 0):,}")
                    formatted_result.append(f"   æ®µæ•¸: {before.get('segments_count', 0)} â†’ {after.get('segments_count', 0)}")
                    
                    before_disk = before.get("disk_size", 0) / 1024 / 1024
                    after_disk = after.get("disk_size", 0) / 1024 / 1024
                    formatted_result.append(f"   ç£ç¢Ÿ: {before_disk:.1f}MB â†’ {after_disk:.1f}MB")
                    
                    before_ram = before.get("ram_size", 0) / 1024 / 1024
                    after_ram = after.get("ram_size", 0) / 1024 / 1024
                    formatted_result.append(f"   è¨˜æ†¶é«”: {before_ram:.1f}MB â†’ {after_ram:.1f}MB")
                    formatted_result.append("")
                    
                    if optimizations:
                        formatted_result.append("ğŸ”§ **æ‡‰ç”¨çš„å„ªåŒ–**")
                        for key, value in optimizations.items():
                            formatted_result.append(f"   â€¢ {key}: {value}")
                        formatted_result.append("")
                
                formatted_result.append("ğŸ”„ **å¾ŒçºŒæ­¥é©Ÿ**")
                formatted_result.append("   å»ºè­°é‡å•Ÿ Qdrant Docker container ä»¥å®Œå…¨æ‡‰ç”¨æ‰€æœ‰å„ªåŒ–:")
                formatted_result.append("   ```bash")
                formatted_result.append("   docker restart <qdrant-container-name>")
                formatted_result.append("   ```")
                
                return formatted_result
                
            except Exception as e:
                logger.error(f"å„²å­˜å„ªåŒ–å¤±æ•—: {e}")
                return [
                    "âŒ **å„²å­˜å„ªåŒ–å¤±æ•—**",
                    f"éŒ¯èª¤: {str(e)}",
                    "",
                    "å¯èƒ½åŸå› :",
                    "â€¢ Qdrant æœå‹™æœªé‹è¡Œ",
                    "â€¢ é€£æ¥é…ç½®éŒ¯èª¤",
                    "â€¢ æ¬Šé™ä¸è¶³"
                ]

        async def analyze_storage(ctx: Context) -> list[str]:
            """
            åˆ†æç•¶å‰ Qdrant çš„å„²å­˜ä½¿ç”¨æƒ…æ³ï¼Œæä¾›å„ªåŒ–å»ºè­°ã€‚
            """
            await ctx.debug("Analyzing Qdrant storage usage")
            
            try:
                analysis = await self.storage_optimizer.get_storage_analysis()
                
                if "error" in analysis:
                    return [
                        "âŒ **å„²å­˜åˆ†æå¤±æ•—**",
                        f"éŒ¯èª¤: {analysis['error']}"
                    ]
                
                result = ["ğŸ“Š **Qdrant å„²å­˜ä½¿ç”¨åˆ†æ**"]
                result.append("")
                
                # ç¸½çµ
                summary = analysis.get("summary", {})
                result.append("ğŸ“ˆ **ç¸½è¨ˆçµ±è¨ˆ**")
                result.append(f"   Collections: {summary.get('total_collections', 0)} å€‹")
                result.append(f"   ç¸½å‘é‡æ•¸: {summary.get('total_vectors', 0):,}")
                result.append(f"   ç¸½ç£ç¢Ÿä½¿ç”¨: {summary.get('total_disk_mb', 0):.1f} MB")
                result.append(f"   ç¸½è¨˜æ†¶é«”ä½¿ç”¨: {summary.get('total_ram_mb', 0):.1f} MB")
                result.append(f"   {summary.get('estimated_optimization_savings', '')}")
                result.append("")
                
                # å„ Collection è©³æƒ…
                collections = analysis.get("collections", [])
                if collections:
                    result.append("ğŸ“ **å„ Collection è©³æƒ…**")
                    for coll in collections:
                        name = coll["collection"]
                        vectors = coll["vectors"]
                        segments = coll["segments"]
                        disk_mb = coll["disk_mb"]
                        ram_mb = coll["ram_mb"]
                        
                        # æ•ˆèƒ½è©•ä¼° - å®‰å…¨åœ°è™•ç† None å€¼
                        disk_mb = coll.get("disk_mb", 0) or 0
                        if disk_mb > 100:
                            perf_icon = "ğŸ”´ å¤§"
                        elif disk_mb > 50:
                            perf_icon = "ğŸŸ¡ ä¸­"
                        else:
                            perf_icon = "ğŸŸ¢ å°"
                        
                        result.append(f"   ğŸ“‚ {name}:")
                        result.append(f"      å‘é‡: {vectors:,}, æ®µ: {segments}")
                        result.append(f"      ç£ç¢Ÿ: {disk_mb:.1f}MB, è¨˜æ†¶é«”: {ram_mb:.1f}MB {perf_icon}")
                        
                        # é…ç½®æª¢æŸ¥
                        config = coll.get("config", {})
                        optimizer = config.get("optimizer", {})
                        hnsw = config.get("hnsw", {})
                        
                        # æª¢æŸ¥æ˜¯å¦éœ€è¦å„ªåŒ– - å®‰å…¨åœ°è™•ç† None å€¼
                        needs_optimization = []
                        max_segment_size = optimizer.get("max_segment_size", 100000) or 100000
                        if max_segment_size > 10000:
                            needs_optimization.append("æ®µå¤§å°éå¤§")
                        
                        memmap_threshold = optimizer.get("memmap_threshold", 10000) or 10000
                        if memmap_threshold > 2000:
                            needs_optimization.append("mmap é–¾å€¼éé«˜")
                        
                        hnsw_m = hnsw.get("m", 16) or 16
                        if hnsw_m > 12:
                            needs_optimization.append("HNSW m å€¼éå¤§")
                            
                        if not hnsw.get("on_disk", False):
                            needs_optimization.append("ç´¢å¼•æœªå­˜ç£ç¢Ÿ")
                        
                        if needs_optimization:
                            result.append(f"      âš ï¸ å»ºè­°å„ªåŒ–: {', '.join(needs_optimization)}")
                        else:
                            result.append(f"      âœ… é…ç½®è‰¯å¥½")
                    result.append("")
                
                # å„ªåŒ–å»ºè­°
                total_disk = summary.get('total_disk_mb', 0) or 0
                if total_disk > 200:
                    result.append("ğŸ’¡ **å„ªåŒ–å»ºè­°**")
                    result.append(f"   ç•¶å‰ç£ç¢Ÿä½¿ç”¨ {total_disk:.1f}MB åé«˜ï¼Œå»ºè­°åŸ·è¡Œå„²å­˜å„ªåŒ–:")
                    result.append("   â€¢ ä½¿ç”¨ qdrant-optimize-storage å·¥å…·å„ªåŒ–æ‰€æœ‰ collections")
                    result.append("   â€¢ é æœŸå¯ç¯€çœ 60-70% ç£ç¢Ÿç©ºé–“")
                    result.append("   â€¢ å„ªåŒ–å¾Œé‡å•Ÿ Qdrant container ä»¥å®Œå…¨ç”Ÿæ•ˆ")
                elif total_disk > 100:
                    result.append("ğŸ’¡ **å„ªåŒ–å»ºè­°**")
                    result.append(f"   ç£ç¢Ÿä½¿ç”¨ {total_disk:.1f}MB ä¸­ç­‰ï¼Œå¯è€ƒæ…®å„ªåŒ–:")
                    result.append("   â€¢ é‡å°å¤§å‹ collections åŸ·è¡Œå„ªåŒ–")
                    result.append("   â€¢ å®šæœŸæ¸…ç†ç„¡ç”¨è³‡æ–™")
                else:
                    result.append("âœ… **å„²å­˜ç‹€æ…‹è‰¯å¥½**")
                    result.append(f"   ç£ç¢Ÿä½¿ç”¨ {total_disk:.1f}MB åœ¨åˆç†ç¯„åœå…§")
                
                return result
                
            except Exception as e:
                logger.error(f"å„²å­˜åˆ†æå¤±æ•—: {e}")
                return [
                    "âŒ **å„²å­˜åˆ†æå¤±æ•—**",
                    f"éŒ¯èª¤: {str(e)}"
                ]

        # è¨»å†Šå„²å­˜å„ªåŒ–å·¥å…·
        if not self.qdrant_settings.read_only:
            self.tool(
                optimize_storage,
                name="qdrant-optimize-storage",
                description="å„ªåŒ– Qdrant collections çš„å„²å­˜é…ç½®ï¼Œæ¸›å°‘ç£ç¢Ÿä½¿ç”¨ç´„ 60-70%ã€‚éœ€è¦æ˜ç¢ºç¢ºèªæ‰èƒ½åŸ·è¡Œã€‚",
            )
        
        self.tool(
            analyze_storage,
            name="qdrant-analyze-storage", 
            description="åˆ†æ Qdrant çš„å„²å­˜ä½¿ç”¨æƒ…æ³ï¼Œæä¾›å„ªåŒ–å»ºè­°ã€‚",
        )

        # RAG Bridge å·¥å…·é›†
        async def search_experience(
            ctx: Context,
            query: Annotated[str, Field(description="æœå°‹å€‹äººç¶“é©—å’ŒçŸ¥è­˜çš„æŸ¥è©¢")],
            content_types: Annotated[
                list[str] | None, 
                Field(description="è¦æœå°‹çš„å…§å®¹é¡å‹ï¼Œå¯é¸: experience, process_workflow, knowledge_base, vocabulary, decision_record")
            ] = None,
            max_results: Annotated[int, Field(description="æœ€å¤šè¿”å›çš„çµæœæ•¸é‡")] = 5,
            min_similarity: Annotated[float, Field(description="æœ€ä½ç›¸ä¼¼åº¦é–¾å€¼")] = 0.7,
            include_experimental: Annotated[bool, Field(description="æ˜¯å¦åŒ…å«å¯¦é©—æ€§å…§å®¹")] = False,
        ) -> list[str]:
            """
            æœå°‹å€‹äººç¶“é©—çŸ¥è­˜åº«ï¼Œæ”¯æ´å¤šç¨®å…§å®¹é¡å‹å’Œæ™ºèƒ½æ’åºã€‚
            """
            await ctx.debug(f"Searching experience for query: {query}")
            
            try:
                # è½‰æ›å…§å®¹é¡å‹
                parsed_content_types = []
                if content_types:
                    for ct in content_types:
                        try:
                            parsed_content_types.append(ContentType(ct))
                        except ValueError:
                            await ctx.debug(f"Invalid content type: {ct}")
                            continue
                
                # å»ºç«‹æœå°‹ä¸Šä¸‹æ–‡
                search_context = SearchContext(
                    query=query,
                    content_types=parsed_content_types if parsed_content_types else None,
                    max_results=max_results,
                    min_similarity=min_similarity,
                    include_experimental=include_experimental,
                )
                
                # åŸ·è¡Œæœå°‹
                results = await self.ragbridge_connector.search_rag_entries(search_context)
                
                if not results:
                    return [f"æ²’æœ‰æ‰¾åˆ°èˆ‡æŸ¥è©¢ '{query}' ç›¸é—œçš„ç¶“é©—çŸ¥è­˜"]
                
                # æ ¼å¼åŒ–çµæœ
                formatted_results = [f"ğŸ” æœå°‹çµæœ '{query}' ({len(results)} å€‹çµæœ):"]
                formatted_results.append("")
                
                for idx, result in enumerate(results, 1):
                    entry = result.entry
                    metadata = entry.metadata
                    
                    formatted_results.append(f"**{idx}. {metadata.title}**")
                    formatted_results.append(f"   ğŸ“ é¡å‹: {metadata.content_type.value}")
                    formatted_results.append(f"   ğŸ¯ ç›¸ä¼¼åº¦: {result.similarity_score:.2f}")
                    formatted_results.append(f"   ğŸ“Š å“è³ª: {metadata.quality_score:.2f}")
                    formatted_results.append(f"   ğŸ“ˆ ä½¿ç”¨æ¬¡æ•¸: {metadata.usage_count}")
                    formatted_results.append(f"   ğŸ·ï¸ æ¨™ç±¤: {', '.join(metadata.tags) if metadata.tags else 'ç„¡'}")
                    
                    # å…§å®¹æ‘˜è¦
                    content_preview = entry.content[:200] + "..." if len(entry.content) > 200 else entry.content
                    formatted_results.append(f"   ğŸ“„ å…§å®¹: {content_preview}")
                    
                    # åŒ¹é…åŸå› 
                    if result.match_reasons:
                        formatted_results.append(f"   ğŸ¯ åŒ¹é…åŸå› : {', '.join(result.match_reasons)}")
                    
                    # ä½¿ç”¨å»ºè­°
                    formatted_results.append(f"   ğŸ’¡ å»ºè­°: {result.usage_recommendation}")
                    formatted_results.append("")
                
                return formatted_results
                
            except Exception as e:
                logger.error(f"Search experience failed: {e}")
                return [f"âŒ æœå°‹ç¶“é©—å¤±æ•—: {str(e)}"]

        async def get_process_workflow(
            ctx: Context,
            workflow_name: Annotated[str, Field(description="å·¥ä½œæµç¨‹åç¨±æˆ–ç›¸é—œé—œéµå­—")],
            include_steps: Annotated[bool, Field(description="æ˜¯å¦åŒ…å«è©³ç´°æ­¥é©Ÿ")] = True,
            include_checkpoints: Annotated[bool, Field(description="æ˜¯å¦åŒ…å«æª¢æŸ¥é»")] = True,
        ) -> list[str]:
            """
            ç²å–ç‰¹å®šæµç¨‹çš„å·¥ä½œæµç¨‹æ­¥é©Ÿï¼Œæ”¯æ´çµæ§‹åŒ–æµç¨‹å±•ç¤ºã€‚
            """
            await ctx.debug(f"Getting process workflow for: {workflow_name}")
            
            try:
                # å»ºç«‹æœå°‹ä¸Šä¸‹æ–‡ï¼Œå°ˆæ³¨æ–¼æµç¨‹å·¥ä½œæµ
                search_context = SearchContext(
                    query=workflow_name,
                    content_types=[ContentType.PROCESS_WORKFLOW],
                    max_results=3,
                    min_similarity=0.6,
                    include_experimental=False,
                )
                
                # åŸ·è¡Œæœå°‹
                results = await self.ragbridge_connector.search_rag_entries(search_context)
                
                if not results:
                    return [f"æ²’æœ‰æ‰¾åˆ° '{workflow_name}' ç›¸é—œçš„å·¥ä½œæµç¨‹"]
                
                # æ ¼å¼åŒ–çµæœ
                formatted_results = [f"ğŸ”„ å·¥ä½œæµç¨‹: {workflow_name}"]
                formatted_results.append("")
                
                for idx, result in enumerate(results, 1):
                    entry = result.entry
                    metadata = entry.metadata
                    
                    formatted_results.append(f"**{idx}. {metadata.title}**")
                    formatted_results.append(f"   ğŸ“Š å“è³ªè©•åˆ†: {metadata.quality_score:.2f}")
                    formatted_results.append(f"   âœ… æˆåŠŸç‡: {metadata.success_rate:.2f}")
                    formatted_results.append(f"   ğŸ·ï¸ æ¨™ç±¤: {', '.join(metadata.tags) if metadata.tags else 'ç„¡'}")
                    formatted_results.append("")
                    
                    # é¡¯ç¤ºæµç¨‹å…§å®¹
                    formatted_results.append("ğŸ“‹ **æµç¨‹å…§å®¹:**")
                    formatted_results.append(entry.content)
                    formatted_results.append("")
                    
                    # é¡¯ç¤ºçµæ§‹åŒ–å…§å®¹
                    if include_steps and entry.structured_content:
                        structured = entry.structured_content
                        
                        if "steps" in structured:
                            formatted_results.append("ğŸ“ **è©³ç´°æ­¥é©Ÿ:**")
                            for step_idx, step in enumerate(structured["steps"], 1):
                                formatted_results.append(f"   {step_idx}. {step}")
                            formatted_results.append("")
                        
                        if include_checkpoints and "checkpoints" in structured:
                            formatted_results.append("ğŸ¯ **æª¢æŸ¥é»:**")
                            for checkpoint in structured["checkpoints"]:
                                formatted_results.append(f"   â€¢ {checkpoint}")
                            formatted_results.append("")
                        
                        if "prerequisites" in structured:
                            formatted_results.append("ğŸ”§ **å‰ç½®éœ€æ±‚:**")
                            for prereq in structured["prerequisites"]:
                                formatted_results.append(f"   â€¢ {prereq}")
                            formatted_results.append("")
                        
                        if "expected_outcomes" in structured:
                            formatted_results.append("ğŸ¯ **é æœŸçµæœ:**")
                            for outcome in structured["expected_outcomes"]:
                                formatted_results.append(f"   â€¢ {outcome}")
                            formatted_results.append("")
                    
                    # èªç¾©å¡Š (å¦‚æœæœ‰)
                    if entry.semantic_chunks:
                        formatted_results.append("ğŸ§© **ç›¸é—œæ¦‚å¿µ:**")
                        for chunk in entry.semantic_chunks[:3]:  # åªé¡¯ç¤ºå‰3å€‹
                            formatted_results.append(f"   â€¢ {chunk}")
                        formatted_results.append("")
                    
                    formatted_results.append(f"   ğŸ’¡ ä½¿ç”¨å»ºè­°: {result.usage_recommendation}")
                    formatted_results.append("")
                
                return formatted_results
                
            except Exception as e:
                logger.error(f"Get process workflow failed: {e}")
                return [f"âŒ ç²å–å·¥ä½œæµç¨‹å¤±æ•—: {str(e)}"]

        async def suggest_similar(
            ctx: Context,
            reference_content: Annotated[str, Field(description="åƒè€ƒå…§å®¹æˆ–æƒ…å¢ƒæè¿°")],
            content_type: Annotated[str, Field(description="å…§å®¹é¡å‹")] = "experience",
            similarity_threshold: Annotated[float, Field(description="ç›¸ä¼¼åº¦é–¾å€¼")] = 0.6,
            max_suggestions: Annotated[int, Field(description="æœ€å¤šå»ºè­°æ•¸é‡")] = 3,
        ) -> list[str]:
            """
            æ ¹æ“šåƒè€ƒå…§å®¹æ¨è–¦ç›¸é—œçš„ç¶“é©—å’ŒçŸ¥è­˜ã€‚
            """
            await ctx.debug(f"Getting similar suggestions for: {reference_content[:50]}...")
            
            try:
                # è½‰æ›å…§å®¹é¡å‹
                try:
                    parsed_content_type = ContentType(content_type)
                except ValueError:
                    parsed_content_type = ContentType.EXPERIENCE
                
                # å»ºç«‹æœå°‹ä¸Šä¸‹æ–‡
                search_context = SearchContext(
                    query=reference_content,
                    content_types=[parsed_content_type],
                    max_results=max_suggestions,
                    min_similarity=similarity_threshold,
                    include_experimental=False,
                )
                
                # åŸ·è¡Œæœå°‹
                results = await self.ragbridge_connector.search_rag_entries(search_context)
                
                if not results:
                    return [f"æ²’æœ‰æ‰¾åˆ°èˆ‡åƒè€ƒå…§å®¹ç›¸ä¼¼çš„ {content_type} å…§å®¹"]
                
                # æ ¼å¼åŒ–çµæœ
                formatted_results = [f"ğŸ”— ç›¸ä¼¼å…§å®¹æ¨è–¦ ({len(results)} å€‹):"]
                formatted_results.append("")
                
                for idx, result in enumerate(results, 1):
                    entry = result.entry
                    metadata = entry.metadata
                    
                    formatted_results.append(f"**{idx}. {metadata.title}**")
                    formatted_results.append(f"   ğŸ¯ ç›¸ä¼¼åº¦: {result.similarity_score:.2f}")
                    formatted_results.append(f"   ğŸ“Š å“è³ª: {metadata.quality_score:.2f}")
                    formatted_results.append(f"   ğŸ“ˆ ä½¿ç”¨æ¬¡æ•¸: {metadata.usage_count}")
                    formatted_results.append(f"   ğŸ·ï¸ æ¨™ç±¤: {', '.join(metadata.tags) if metadata.tags else 'ç„¡'}")
                    
                    # å…§å®¹æ‘˜è¦
                    content_preview = entry.content[:150] + "..." if len(entry.content) > 150 else entry.content
                    formatted_results.append(f"   ğŸ“„ æ‘˜è¦: {content_preview}")
                    
                    # ç›¸ä¼¼æ€§åŸå› 
                    if result.match_reasons:
                        formatted_results.append(f"   ğŸ¯ ç›¸ä¼¼åŸå› : {', '.join(result.match_reasons)}")
                    
                    # æ‡‰ç”¨å»ºè­°
                    formatted_results.append(f"   ğŸ’¡ å¦‚ä½•æ‡‰ç”¨: {result.usage_recommendation}")
                    formatted_results.append("")
                
                return formatted_results
                
            except Exception as e:
                logger.error(f"Suggest similar failed: {e}")
                return [f"âŒ æ¨è–¦ç›¸ä¼¼å…§å®¹å¤±æ•—: {str(e)}"]

        async def update_experience(
            ctx: Context,
            content_id: Annotated[str, Field(description="å…§å®¹ID")],
            content_type: Annotated[str, Field(description="å…§å®¹é¡å‹")] = "experience",
            feedback_type: Annotated[str, Field(description="åé¥‹é¡å‹: success, failure, improvement")] = "success",
            feedback_notes: Annotated[str, Field(description="åé¥‹è©³ç´°èªªæ˜")] = "",
            quality_adjustment: Annotated[float, Field(description="å“è³ªèª¿æ•´ (-1.0 åˆ° 1.0)")] = 0.0,
        ) -> str:
            """
            æ›´æ–°ç¶“é©—åé¥‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨çµ±è¨ˆå’Œå“è³ªè©•åˆ†ã€‚
            """
            await ctx.debug(f"Updating experience feedback for: {content_id}")
            
            try:
                # è½‰æ›å…§å®¹é¡å‹
                try:
                    parsed_content_type = ContentType(content_type)
                except ValueError:
                    return f"âŒ ç„¡æ•ˆçš„å…§å®¹é¡å‹: {content_type}"
                
                # ç²å–ç¾æœ‰å…§å®¹
                existing_content = await self.ragbridge_connector.get_content_by_id(
                    content_id, parsed_content_type
                )
                
                if not existing_content:
                    return f"âŒ æ‰¾ä¸åˆ°å…§å®¹ ID: {content_id}"
                
                # æº–å‚™æ›´æ–°è³‡æ–™
                current_metadata = existing_content.metadata
                updates = {
                    "updated_at": datetime.now().isoformat(),
                    "usage_count": current_metadata.usage_count + 1,
                }
                
                # æ ¹æ“šåé¥‹é¡å‹æ›´æ–°çµ±è¨ˆ
                if feedback_type == "success":
                    new_success_count = getattr(current_metadata, 'success_count', 0) + 1
                    total_usage = updates["usage_count"]
                    updates["success_rate"] = new_success_count / total_usage if total_usage > 0 else 0.0
                    updates["success_count"] = new_success_count
                elif feedback_type == "failure":
                    failure_count = getattr(current_metadata, 'failure_count', 0) + 1
                    updates["failure_count"] = failure_count
                    success_count = getattr(current_metadata, 'success_count', 0)
                    total_usage = updates["usage_count"]
                    updates["success_rate"] = success_count / total_usage if total_usage > 0 else 0.0
                
                # èª¿æ•´å“è³ªåˆ†æ•¸
                if quality_adjustment != 0.0:
                    new_quality = max(0.0, min(1.0, current_metadata.quality_score + quality_adjustment))
                    updates["quality_score"] = new_quality
                
                # æ·»åŠ åé¥‹è¨˜éŒ„
                if feedback_notes:
                    feedback_history = getattr(current_metadata, 'feedback_history', [])
                    feedback_history.append({
                        "timestamp": datetime.now().isoformat(),
                        "type": feedback_type,
                        "notes": feedback_notes,
                        "quality_adjustment": quality_adjustment,
                    })
                    updates["feedback_history"] = feedback_history
                
                # æ›´æ–°å…§å®¹
                await self.ragbridge_connector.update_content_metadata(
                    content_id, parsed_content_type, updates
                )
                
                return f"âœ… å·²æ›´æ–° {content_id} çš„åé¥‹è³‡æ–™ (é¡å‹: {feedback_type})"
                
            except Exception as e:
                logger.error(f"Update experience failed: {e}")
                return f"âŒ æ›´æ–°ç¶“é©—åé¥‹å¤±æ•—: {str(e)}"

        # è¨»å†Š RAG Bridge å·¥å…·
        self.tool(
            search_experience,
            name="search-experience",
            description="æœå°‹å€‹äººç¶“é©—çŸ¥è­˜åº«ï¼Œæ”¯æ´å¤šç¨®å…§å®¹é¡å‹å’Œæ™ºèƒ½æ’åº",
        )
        
        self.tool(
            get_process_workflow,
            name="get-process-workflow",
            description="ç²å–ç‰¹å®šæµç¨‹çš„å·¥ä½œæµç¨‹æ­¥é©Ÿï¼Œæ”¯æ´çµæ§‹åŒ–æµç¨‹å±•ç¤º",
        )
        
        self.tool(
            suggest_similar,
            name="suggest-similar",
            description="æ ¹æ“šåƒè€ƒå…§å®¹æ¨è–¦ç›¸é—œçš„ç¶“é©—å’ŒçŸ¥è­˜",
        )
        
        # åªåœ¨éå”¯è®€æ¨¡å¼ä¸‹è¨»å†Šæ›´æ–°å·¥å…·
        if not self.qdrant_settings.read_only:
            self.tool(
                update_experience,
                name="update-experience",
                description="æ›´æ–°ç¶“é©—åé¥‹ï¼ŒåŒ…æ‹¬ä½¿ç”¨çµ±è¨ˆå’Œå“è³ªè©•åˆ†",
            )

        # è©å½™ç®¡ç†å·¥å…·é›† (Task 141)
        async def search_vocabulary(
            ctx: Context,
            query: Annotated[str, Field(description="æœå°‹è©å½™çš„æŸ¥è©¢å­—ä¸²")] = "",
            domain: Annotated[str | None, Field(description="è©å½™é ˜åŸŸéæ¿¾")] = None,
            status: Annotated[str | None, Field(description="è©å½™ç‹€æ…‹éæ¿¾")] = None,
            limit: Annotated[int, Field(description="æœ€å¤šè¿”å›çµæœæ•¸é‡")] = 10,
        ) -> list[str]:
            """
            æœå°‹å’Œç€è¦½æ¨™æº–åŒ–è©å½™åº«ï¼Œæ”¯æ´é ˜åŸŸå’Œç‹€æ…‹éæ¿¾ã€‚
            """
            await ctx.debug(f"Searching vocabulary for: {query}")
            
            try:
                result = await vocabulary_api.search_vocabulary(
                    query=query,
                    domain=domain,
                    status=status,
                    limit=limit
                )
                
                if "error" in result:
                    return [f"âŒ æœå°‹è©å½™å¤±æ•—: {result['error']}"]
                
                formatted_result = [f"ğŸ” è©å½™æœå°‹çµæœ '{query}' ({result['total_results']} å€‹çµæœ):"]
                formatted_result.append("")
                
                for idx, vocab in enumerate(result['results'], 1):
                    formatted_result.append(f"**{idx}. {vocab['term']}** ({vocab['match_type']})")
                    formatted_result.append(f"   ğŸ“‚ é ˜åŸŸ: {vocab['domain']}")
                    formatted_result.append(f"   ğŸ“Š ç‹€æ…‹: {vocab['status']}")
                    formatted_result.append(f"   ğŸ“ˆ ä½¿ç”¨æ¬¡æ•¸: {vocab['usage_count']}")
                    
                    if vocab['synonyms']:
                        formatted_result.append(f"   ğŸ”— åŒç¾©è©: {', '.join(vocab['synonyms'])}")
                    
                    if vocab['definition']:
                        formatted_result.append(f"   ğŸ“ å®šç¾©: {vocab['definition']}")
                    
                    formatted_result.append("")
                
                return formatted_result
                
            except Exception as e:
                logger.error(f"Search vocabulary failed: {e}")
                return [f"âŒ æœå°‹è©å½™å¤±æ•—: {str(e)}"]

        async def propose_vocabulary(
            ctx: Context,
            term: Annotated[str, Field(description="æè­°çš„æ–°è©å½™")],
            domain: Annotated[str, Field(description="è©å½™æ‰€å±¬é ˜åŸŸ")],
            definition: Annotated[str, Field(description="è©å½™å®šç¾©")] = "",
            synonyms: Annotated[list[str] | None, Field(description="åŒç¾©è©åˆ—è¡¨")] = None,
        ) -> str:
            """
            æè­°æ–°çš„æ¨™æº–åŒ–è©å½™é …ç›®ï¼Œéœ€è¦å¾ŒçºŒå¯©æ ¸æ‰¹å‡†ã€‚
            """
            await ctx.debug(f"Proposing new vocabulary term: {term}")
            
            try:
                result = await vocabulary_api.propose_vocabulary(
                    term=term,
                    domain=domain,
                    definition=definition,
                    synonyms=synonyms or []
                )
                
                if result['success']:
                    return f"âœ… {result['message']}"
                else:
                    return f"âŒ {result['message']}"
                    
            except Exception as e:
                logger.error(f"Propose vocabulary failed: {e}")
                return f"âŒ æè­°è©å½™å¤±æ•—: {str(e)}"

        async def standardize_content(
            ctx: Context,
            content: Annotated[str, Field(description="è¦æ¨™æº–åŒ–çš„å…§å®¹æ–‡æœ¬")],
            tags: Annotated[list[str] | None, Field(description="å…§å®¹æ¨™ç±¤åˆ—è¡¨")] = None,
        ) -> list[str]:
            """
            æ¨™æº–åŒ–å…§å®¹å’Œæ¨™ç±¤ï¼Œæä¾›è©å½™å»ºè­°å’Œå„ªåŒ–ã€‚
            """
            await ctx.debug(f"Standardizing content: {content[:100]}...")
            
            try:
                result = await vocabulary_api.standardize_content(
                    content=content,
                    tags=tags or []
                )
                
                if "error" in result:
                    return [f"âŒ æ¨™æº–åŒ–å¤±æ•—: {result['error']}"]
                
                formatted_result = ["ğŸ”§ å…§å®¹æ¨™æº–åŒ–çµæœ:"]
                formatted_result.append("")
                
                # æ¨™ç±¤æ¨™æº–åŒ–
                formatted_result.append("ğŸ·ï¸ **æ¨™ç±¤æ¨™æº–åŒ–:**")
                if result['original_tags']:
                    formatted_result.append(f"   åŸå§‹: {', '.join(result['original_tags'])}")
                formatted_result.append(f"   æ¨™æº–åŒ–: {', '.join(result['standardized_tags'])}")
                
                if result['suggested_additional_tags']:
                    formatted_result.append(f"   å»ºè­°æ–°å¢: {', '.join(result['suggested_additional_tags'])}")
                formatted_result.append("")
                
                # è©å½™å»ºè­°
                if result['vocabulary_suggestions']:
                    formatted_result.append("ğŸ’¡ **è©å½™æ¨™æº–åŒ–å»ºè­°:**")
                    for suggestion in result['vocabulary_suggestions']:
                        formatted_result.append(f"   â€¢ '{suggestion['original']}' â†’ '{suggestion['suggested']}' ({suggestion['reason']})")
                    formatted_result.append("")
                
                # ç›¸é—œè©å½™
                if result['related_terms']:
                    formatted_result.append("ğŸ”— **ç›¸é—œè©å½™:**")
                    formatted_result.append(f"   {', '.join(result['related_terms'])}")
                
                return formatted_result
                
            except Exception as e:
                logger.error(f"Standardize content failed: {e}")
                return [f"âŒ æ¨™æº–åŒ–å…§å®¹å¤±æ•—: {str(e)}"]

        async def get_vocabulary_statistics(ctx: Context) -> list[str]:
            """
            ç²å–è©å½™ç®¡ç†ç³»çµ±çš„çµ±è¨ˆè³‡è¨Šå’Œå¥åº·ç‹€æ…‹ã€‚
            """
            await ctx.debug("Getting vocabulary statistics")
            
            try:
                result = await vocabulary_api.get_vocabulary_statistics()
                
                if "error" in result:
                    return [f"âŒ ç²å–çµ±è¨ˆå¤±æ•—: {result['error']}"]
                
                vocab_stats = result['vocabulary_statistics']
                fragment_stats = result['fragment_statistics']
                health = result['system_health']
                
                formatted_result = ["ğŸ“Š **è©å½™ç®¡ç†ç³»çµ±çµ±è¨ˆ**"]
                formatted_result.append("")
                
                # è©å½™çµ±è¨ˆ
                formatted_result.append("ğŸ“š **è©å½™åº«çµ±è¨ˆ:**")
                formatted_result.append(f"   ç¸½è©å½™æ•¸: {vocab_stats['total_terms']}")
                formatted_result.append(f"   åŒç¾©è©æ•¸: {vocab_stats['total_synonyms']}")
                formatted_result.append(f"   ç¸½ä½¿ç”¨æ¬¡æ•¸: {vocab_stats['total_usage']}")
                formatted_result.append(f"   å¹³å‡ä½¿ç”¨æ¬¡æ•¸: {vocab_stats['average_usage']:.1f}")
                formatted_result.append("")
                
                # é ˜åŸŸåˆ†ä½ˆ
                formatted_result.append("ğŸ—‚ï¸ **é ˜åŸŸåˆ†ä½ˆ:**")
                for domain, count in vocab_stats['domain_distribution'].items():
                    if count > 0:
                        formatted_result.append(f"   {domain}: {count} å€‹è©å½™")
                formatted_result.append("")
                
                # ç‹€æ…‹åˆ†ä½ˆ
                formatted_result.append("ğŸ“ˆ **è©å½™ç‹€æ…‹:**")
                for status, count in vocab_stats['status_distribution'].items():
                    if count > 0:
                        formatted_result.append(f"   {status}: {count} å€‹è©å½™")
                formatted_result.append("")
                
                # æœ€å¸¸ç”¨è©å½™
                if vocab_stats['most_used_terms']:
                    formatted_result.append("ğŸ”¥ **æœ€å¸¸ç”¨è©å½™:**")
                    for term in vocab_stats['most_used_terms'][:5]:
                        formatted_result.append(f"   â€¢ {term['term']} ({term['domain']}) - {term['usage_count']} æ¬¡")
                    formatted_result.append("")
                
                # åˆ†ç‰‡çµ±è¨ˆ
                formatted_result.append("ğŸ“„ **åˆ†ç‰‡çµ±è¨ˆ:**")
                formatted_result.append(f"   ç¸½åˆ†ç‰‡æ•¸: {fragment_stats['total_fragments']}")
                formatted_result.append(f"   å¹³å‡å“è³ª: {fragment_stats['average_quality']:.2f}")
                formatted_result.append(f"   ç¸½ä½¿ç”¨æ¬¡æ•¸: {fragment_stats['total_usage']}")
                formatted_result.append("")
                
                # ç³»çµ±å¥åº·
                formatted_result.append("ğŸ’š **ç³»çµ±å¥åº·:**")
                formatted_result.append(f"   è©å½™è¦†è“‹ç‡: {health['vocabulary_coverage']:.1%}")
                formatted_result.append(f"   ä½¿ç”¨æ´»èºåº¦: {health['usage_activity']:.1%}")
                formatted_result.append(f"   é ˜åŸŸå¤šæ¨£æ€§: {health['domain_diversity']:.1%}")
                
                return formatted_result
                
            except Exception as e:
                logger.error(f"Get vocabulary statistics failed: {e}")
                return [f"âŒ ç²å–çµ±è¨ˆå¤±æ•—: {str(e)}"]

        async def manage_fragment_vocabulary(
            ctx: Context,
            action: Annotated[str, Field(description="æ“ä½œé¡å‹: search, create, analyze")],
            fragment_type: Annotated[str | None, Field(description="åˆ†ç‰‡é¡å‹éæ¿¾")] = None,
            query: Annotated[str, Field(description="æœå°‹æŸ¥è©¢")] = "",
            limit: Annotated[int, Field(description="çµæœé™åˆ¶")] = 5,
        ) -> list[str]:
            """
            ç®¡ç†åˆ†ç‰‡è©å½™ï¼ŒåŒ…æ‹¬æœå°‹ã€å‰µå»ºå’Œåˆ†æåˆ†ç‰‡ã€‚
            """
            await ctx.debug(f"Managing fragment vocabulary: {action}")
            
            try:
                if action == "search":
                    # æœå°‹åˆ†ç‰‡
                    search_params = {"limit": limit}
                    if query:
                        search_params["query"] = query
                    if fragment_type:
                        # é€™è£¡éœ€è¦è½‰æ›å­—ç¬¦ä¸²åˆ°æšèˆ‰
                        from mcp_server_qdrant.ragbridge.vocabulary import FragmentType
                        try:
                            ftype = FragmentType(fragment_type)
                            search_params["fragment_types"] = [ftype]
                        except ValueError:
                            return [f"âŒ ç„¡æ•ˆçš„åˆ†ç‰‡é¡å‹: {fragment_type}"]
                    
                    results = fragment_manager.search_fragments(**search_params)
                    
                    formatted_result = [f"ğŸ” åˆ†ç‰‡æœå°‹çµæœ ({len(results)} å€‹):"]
                    formatted_result.append("")
                    
                    for idx, item in enumerate(results, 1):
                        fragment = item['fragment']
                        score = item['relevance_score']
                        
                        formatted_result.append(f"**{idx}. {fragment['title']}**")
                        formatted_result.append(f"   ğŸ“ é¡å‹: {fragment['fragment_type']}")
                        formatted_result.append(f"   ğŸ¯ ç›¸é—œæ€§: {score:.2f}")
                        formatted_result.append(f"   ğŸ“Š å“è³ª: {fragment['quality_score']:.2f}")
                        formatted_result.append(f"   ğŸ·ï¸ æ¨™ç±¤: {', '.join(fragment['tags'])}")
                        formatted_result.append(f"   ğŸ“ˆ ä½¿ç”¨: {fragment['usage_count']} æ¬¡")
                        
                        # ç›¸é—œåˆ†ç‰‡
                        if item['related_fragments']:
                            related_info = [f"{r[1]}({r[2]:.1f})" for r in item['related_fragments'][:2]]
                            formatted_result.append(f"   ğŸ”— ç›¸é—œ: {', '.join(related_info)}")
                        
                        formatted_result.append("")
                
                elif action == "analyze":
                    # åˆ†æåˆ†ç‰‡çµ±è¨ˆ
                    stats = fragment_manager.get_fragment_statistics()
                    
                    formatted_result = ["ğŸ“Š **åˆ†ç‰‡è©å½™åˆ†æ**"]
                    formatted_result.append("")
                    
                    formatted_result.append(f"ğŸ“„ ç¸½åˆ†ç‰‡æ•¸: {stats['total_fragments']}")
                    formatted_result.append(f"ğŸ“ˆ ç¸½ä½¿ç”¨æ¬¡æ•¸: {stats['total_usage']}")
                    formatted_result.append(f"ğŸ“Š å¹³å‡å“è³ª: {stats['average_quality']:.2f}")
                    formatted_result.append(f"ğŸ·ï¸ ç¸½æ¨™ç±¤æ•¸: {stats['total_tags']}")
                    formatted_result.append(f"ğŸ”‘ ç¸½é—œéµè©: {stats['total_keywords']}")
                    formatted_result.append("")
                    
                    formatted_result.append("ğŸ“ **é¡å‹åˆ†ä½ˆ:**")
                    for ftype, count in stats['type_distribution'].items():
                        if count > 0:
                            formatted_result.append(f"   {ftype}: {count}")
                    formatted_result.append("")
                    
                    formatted_result.append("ğŸ—‚ï¸ **é ˜åŸŸåˆ†ä½ˆ:**")
                    for domain, count in stats['domain_distribution'].items():
                        if count > 0:
                            formatted_result.append(f"   {domain}: {count}")
                
                else:
                    return [f"âŒ ä¸æ”¯æ´çš„æ“ä½œ: {action}"]
                
                return formatted_result
                
            except Exception as e:
                logger.error(f"Manage fragment vocabulary failed: {e}")
                return [f"âŒ ç®¡ç†åˆ†ç‰‡è©å½™å¤±æ•—: {str(e)}"]

        # è¨»å†Šè©å½™ç®¡ç†å·¥å…·
        self.tool(
            search_vocabulary,
            name="search-vocabulary",
            description="æœå°‹å’Œç€è¦½æ¨™æº–åŒ–è©å½™åº«ï¼Œæ”¯æ´é ˜åŸŸå’Œç‹€æ…‹éæ¿¾",
        )
        
        self.tool(
            standardize_content,
            name="standardize-content",
            description="æ¨™æº–åŒ–å…§å®¹å’Œæ¨™ç±¤ï¼Œæä¾›è©å½™å»ºè­°å’Œå„ªåŒ–",
        )
        
        self.tool(
            get_vocabulary_statistics,
            name="get-vocabulary-statistics",
            description="ç²å–è©å½™ç®¡ç†ç³»çµ±çš„çµ±è¨ˆè³‡è¨Šå’Œå¥åº·ç‹€æ…‹",
        )
        
        self.tool(
            manage_fragment_vocabulary,
            name="manage-fragment-vocabulary",
            description="ç®¡ç†åˆ†ç‰‡è©å½™ï¼ŒåŒ…æ‹¬æœå°‹ã€å‰µå»ºå’Œåˆ†æåˆ†ç‰‡",
        )
        
        # åªåœ¨éå”¯è®€æ¨¡å¼ä¸‹è¨»å†Šç·¨è¼¯å·¥å…·
        if not self.qdrant_settings.read_only:
            self.tool(
                propose_vocabulary,
                name="propose-vocabulary",
                description="æè­°æ–°çš„æ¨™æº–åŒ–è©å½™é …ç›®ï¼Œéœ€è¦å¾ŒçºŒå¯©æ ¸æ‰¹å‡†",
            )

        # Schema ç®¡ç†å·¥å…·é›† (Task 142)
        async def get_current_schema(
            ctx: Context,
        ) -> list[str]:
            """
            ç²å–ç•¶å‰æ´»èºçš„ Schema ç‰ˆæœ¬åŠå…¶è©³ç´°è³‡è¨Šã€‚
            """
            await ctx.debug("Getting current schema")
            
            try:
                result = await schema_api.get_current_schema()
                
                if "error" in result:
                    return [f"âŒ ç²å– Schema å¤±æ•—: {result['error']}"]
                
                # æ ¼å¼åŒ–è¼¸å‡º
                output = [
                    f"ğŸ“‹ **ç•¶å‰ Schema ç‰ˆæœ¬: {result['schema_version']}**",
                    f"ğŸ“ æè¿°: {result['description']}",
                    f"ğŸ“… å»ºç«‹æ™‚é–“: {result['created_at']}",
                    f"ğŸ”„ æ´»èºç‹€æ…‹: {'æ˜¯' if result['is_active'] else 'å¦'}",
                    f"ğŸ”— å‘å¾Œå…¼å®¹: {'æ˜¯' if result['backward_compatible'] else 'å¦'}",
                    "",
                    f"ğŸ“Š **çµ±è¨ˆè³‡è¨Š:**",
                    f"- ç¸½æ¬„ä½æ•¸: {result['total_fields']}",
                    f"- æ ¸å¿ƒæ¬„ä½æ•¸: {result['core_fields_count']}",
                    f"- å·²æ£„ç”¨æ¬„ä½æ•¸: {result['deprecated_fields_count']}",
                    "",
                    f"ğŸ—ï¸ **æ¬„ä½å®šç¾©:**"
                ]
                
                for field_name, field_info in result['fields'].items():
                    status = "ğŸ”´ (å·²æ£„ç”¨)" if field_info['deprecated'] else "âœ…"
                    core = "ğŸ”’ (æ ¸å¿ƒ)" if field_info['is_core'] else ""
                    required = "âš ï¸ (å¿…å¡«)" if field_info['required'] else ""
                    
                    output.append(f"- **{field_name}** {status} {core} {required}")
                    output.append(f"  - é¡å‹: {field_info['type']}")
                    if field_info['description']:
                        output.append(f"  - æè¿°: {field_info['description']}")
                    output.append(f"  - æ–°å¢æ–¼ç‰ˆæœ¬: {field_info['added_in_version']}")
                    output.append("")
                
                return output
                
            except Exception as e:
                logger.error(f"Get current schema failed: {e}")
                return [f"âŒ ç²å–ç•¶å‰ Schema å¤±æ•—: {str(e)}"]

        async def request_schema_field_addition(
            ctx: Context,
            field_name: Annotated[str, Field(description="æ¬„ä½åç¨±")],
            field_type: Annotated[str, Field(description="æ¬„ä½é¡å‹ (string, integer, float, boolean, datetime, list, dict, json)")],
            description: Annotated[str, Field(description="æ¬„ä½æè¿°")] = "",
            required: Annotated[bool, Field(description="æ˜¯å¦ç‚ºå¿…å¡«æ¬„ä½")] = False,
            justification: Annotated[str, Field(description="è®Šæ›´ç†ç”±èªªæ˜")] = "",
            proposed_by: Annotated[str, Field(description="ææ¡ˆè€…èº«ä»½")] = "mcp_user",
            min_length: Annotated[int | None, Field(description="æœ€å°é•·åº¦é™åˆ¶")] = None,
            max_length: Annotated[int | None, Field(description="æœ€å¤§é•·åº¦é™åˆ¶")] = None,
            pattern: Annotated[str | None, Field(description="æ­£å‰‡è¡¨é”å¼æ¨¡å¼")] = None,
            min_value: Annotated[float | None, Field(description="æœ€å°å€¼é™åˆ¶")] = None,
            max_value: Annotated[float | None, Field(description="æœ€å¤§å€¼é™åˆ¶")] = None,
            allowed_values: Annotated[list[str] | None, Field(description="å…è¨±çš„å€¼åˆ—è¡¨")] = None,
        ) -> list[str]:
            """
            è«‹æ±‚æ–°å¢ Schema æ¬„ä½ï¼Œå°‡å‰µå»ºå¯©æŸ¥è«‹æ±‚è€Œéç›´æ¥åŸ·è¡Œè®Šæ›´ã€‚
            """
            await ctx.debug(f"Requesting schema field addition: {field_name}")
            
            try:
                # çµ„å»ºè®Šæ›´è©³æƒ…
                change_details = {
                    "field_type": field_type,
                    "description": description,
                    "required": required
                }
                
                # çµ„å»ºé©—è­‰è¦å‰‡
                validation_rules = {}
                if min_length is not None:
                    validation_rules["min_length"] = min_length
                if max_length is not None:
                    validation_rules["max_length"] = max_length
                if pattern is not None:
                    validation_rules["pattern"] = pattern
                if min_value is not None:
                    validation_rules["min_value"] = min_value
                if max_value is not None:
                    validation_rules["max_value"] = max_value
                if allowed_values is not None:
                    validation_rules["allowed_values"] = allowed_values
                
                if validation_rules:
                    change_details["validation"] = validation_rules
                
                # å‰µå»ºå¯©æŸ¥è«‹æ±‚
                approval_manager = get_approval_manager()
                request_id = approval_manager.create_change_request(
                    change_type="add_field",
                    field_name=field_name,
                    change_details=change_details,
                    proposed_by=proposed_by,
                    justification=justification
                )
                
                # æª¢æŸ¥æ˜¯å¦å·²è‡ªå‹•æ ¸å‡†
                if request_id in approval_manager.approval_history:
                    # å·²è‡ªå‹•åŸ·è¡Œ
                    executed_request = next(
                        req for req in approval_manager.approval_history 
                        if req.request_id == request_id
                    )
                    
                    if executed_request.status == "approved":
                        return [
                            f"âœ… ä½é¢¨éšªè®Šæ›´å·²è‡ªå‹•æ ¸å‡†ä¸¦åŸ·è¡Œ",
                            f"ğŸ“‹ è«‹æ±‚ID: {request_id}",
                            f"ğŸ—ï¸ æ–°å¢æ¬„ä½: {field_name} ({field_type})",
                            f"ğŸ“ èªªæ˜: {executed_request.review_comments}"
                        ]
                    else:
                        return [
                            f"âŒ è‡ªå‹•åŸ·è¡Œå¤±æ•—",
                            f"ğŸ“‹ è«‹æ±‚ID: {request_id}",
                            f"ğŸ’¬ éŒ¯èª¤: {executed_request.review_comments}"
                        ]
                else:
                    # ç­‰å¾…å¯©æŸ¥
                    pending_request = approval_manager.pending_requests[request_id]
                    return [
                        f"ğŸ“‹ **Schema è®Šæ›´è«‹æ±‚å·²å‰µå»º**",
                        f"ğŸ†” è«‹æ±‚ID: {request_id}",
                        f"ğŸ—ï¸ è®Šæ›´é¡å‹: æ–°å¢æ¬„ä½ '{field_name}'",
                        f"âš ï¸ é¢¨éšªç´šåˆ¥: {pending_request.risk_level.value}",
                        f"ğŸ‘¥ éœ€è¦å¯©æŸ¥ç´šåˆ¥: {pending_request.required_approval_level.value}",
                        f"ğŸ“ ææ¡ˆç†ç”±: {justification}",
                        "",
                        f"â³ **ç‹€æ…‹: ç­‰å¾…å¯©æŸ¥**",
                        f"ğŸ’¡ ä½¿ç”¨ 'review-schema-request' å·¥å…·é€²è¡Œå¯©æŸ¥",
                        f"ğŸ” ä½¿ç”¨ 'list-pending-schema-requests' æŸ¥çœ‹æ‰€æœ‰å¾…å¯©æŸ¥è«‹æ±‚"
                    ]
                
            except Exception as e:
                logger.error(f"Request schema field addition failed: {e}")
                return [f"âŒ å‰µå»º Schema è®Šæ›´è«‹æ±‚å¤±æ•—: {str(e)}"]

        async def request_schema_field_removal(
            ctx: Context,
            field_name: Annotated[str, Field(description="è¦ç§»é™¤çš„æ¬„ä½åç¨±")],
            justification: Annotated[str, Field(description="ç§»é™¤ç†ç”±èªªæ˜")] = "",
            proposed_by: Annotated[str, Field(description="ææ¡ˆè€…èº«ä»½")] = "mcp_user",
        ) -> list[str]:
            """
            è«‹æ±‚ç§»é™¤ Schema æ¬„ä½ï¼Œå°‡å‰µå»ºå¯©æŸ¥è«‹æ±‚ï¼ˆé«˜é¢¨éšªæ“ä½œï¼‰ã€‚
            """
            await ctx.debug(f"Requesting schema field removal: {field_name}")
            
            try:
                # å‰µå»ºå¯©æŸ¥è«‹æ±‚
                approval_manager = get_approval_manager()
                request_id = approval_manager.create_change_request(
                    change_type="remove_field",
                    field_name=field_name,
                    change_details={"deprecated": True},
                    proposed_by=proposed_by,
                    justification=justification
                )
                
                # ç§»é™¤æ¬„ä½æ˜¯é«˜é¢¨éšªæ“ä½œï¼Œä¸æœƒè‡ªå‹•æ ¸å‡†
                pending_request = approval_manager.pending_requests[request_id]
                return [
                    f"âš ï¸ **é«˜é¢¨éšª Schema è®Šæ›´è«‹æ±‚å·²å‰µå»º**",
                    f"ğŸ†” è«‹æ±‚ID: {request_id}",
                    f"ğŸ—‘ï¸ è®Šæ›´é¡å‹: ç§»é™¤æ¬„ä½ '{field_name}'",
                    f"ğŸ”´ é¢¨éšªç´šåˆ¥: {pending_request.risk_level.value}",
                    f"ğŸ‘¥ éœ€è¦å¯©æŸ¥ç´šåˆ¥: {pending_request.required_approval_level.value}",
                    f"ğŸ“ ç§»é™¤ç†ç”±: {justification}",
                    "",
                    f"â³ **ç‹€æ…‹: ç­‰å¾…é«˜ç´šå¯©æŸ¥**",
                    f"ğŸ’¡ éœ€è¦ç®¡ç†å“¡æ¬Šé™é€²è¡Œå¯©æŸ¥",
                    f"ğŸ” ä½¿ç”¨ 'review-schema-request' å·¥å…·é€²è¡Œå¯©æŸ¥"
                ]
                
            except Exception as e:
                logger.error(f"Request schema field removal failed: {e}")
                return [f"âŒ å‰µå»ºç§»é™¤æ¬„ä½è«‹æ±‚å¤±æ•—: {str(e)}"]

        async def validate_schema_data(
            ctx: Context,
            data: Annotated[dict, Field(description="è¦é©—è­‰çš„æ•¸æ“š")],
            schema_version: Annotated[str | None, Field(description="æŒ‡å®šçš„ Schema ç‰ˆæœ¬")] = None,
        ) -> list[str]:
            """
            é©—è­‰æ•¸æ“šæ˜¯å¦ç¬¦åˆ Schema è¦ç¯„ã€‚
            """
            await ctx.debug(f"Validating data against schema version: {schema_version or 'current'}")
            
            try:
                result = await schema_api.validate_data(data, schema_version)
                
                output = [
                    f"ğŸ” **Schema é©—è­‰çµæœ**",
                    f"ğŸ“‹ ä½¿ç”¨ Schema ç‰ˆæœ¬: {result['schema_version']}",
                    f"âœ… é©—è­‰çµæœ: {'é€šé' if result['is_valid'] else 'å¤±æ•—'}",
                    f"âŒ éŒ¯èª¤æ•¸é‡: {result['error_count']}",
                    f"ğŸ’¬ {result['message']}"
                ]
                
                if result["validation_errors"]:
                    output.append("")
                    output.append("ğŸš¨ **é©—è­‰éŒ¯èª¤è©³æƒ…:**")
                    for i, error in enumerate(result["validation_errors"], 1):
                        output.append(f"{i}. {error}")
                
                return output
                
            except Exception as e:
                logger.error(f"Validate schema data failed: {e}")
                return [f"âŒ Schema æ•¸æ“šé©—è­‰å¤±æ•—: {str(e)}"]

        async def analyze_schema_usage(
            ctx: Context,
            data_samples: Annotated[list[dict], Field(description="ç”¨æ–¼åˆ†æçš„æ•¸æ“šæ¨£æœ¬åˆ—è¡¨")],
        ) -> list[str]:
            """
            åˆ†æ Schema ä½¿ç”¨æƒ…æ³ï¼Œæä¾›å„ªåŒ–å»ºè­°ã€‚
            """
            await ctx.debug(f"Analyzing schema usage with {len(data_samples)} samples")
            
            try:
                result = await schema_api.analyze_schema_usage(data_samples)
                
                if "error" in result:
                    return [f"âŒ {result['error']}"]
                
                summary = result.get("summary", {})
                
                output = [
                    f"ğŸ“Š **Schema ä½¿ç”¨æƒ…æ³åˆ†æ**",
                    f"ğŸ“‹ ç•¶å‰ Schema ç‰ˆæœ¬: {result['current_schema_version']}",
                    f"ğŸ“¦ åˆ†ææ¨£æœ¬æ•¸é‡: {result['total_samples']}",
                    f"âœ… Schema åˆè¦ç‡: {result['schema_compliance_rate']:.1%}",
                    f"ğŸ¯ åˆè¦ç­‰ç´š: {summary.get('compliance_level', 'unknown')}",
                    "",
                    f"ğŸ“ˆ **é«˜ä½¿ç”¨ç‡æ¬„ä½ (>80%):**"
                ]
                
                for field in summary.get("high_usage_fields", []):
                    output.append(f"  - {field}")
                
                output.append("")
                output.append(f"ğŸ“‰ **ä½ä½¿ç”¨ç‡æ¬„ä½ (<20%):**")
                for field in summary.get("low_usage_fields", []):
                    output.append(f"  - {field}")
                
                if result.get("unknown_fields"):
                    output.append("")
                    output.append(f"ğŸ” **æœªå®šç¾©ä½†å¸¸ç”¨çš„æ¬„ä½:**")
                    for field in result["unknown_fields"]:
                        output.append(f"  - {field}")
                
                if summary.get("suggestions_available"):
                    output.append("")
                    output.append("ğŸ’¡ å»ºè­°ä½¿ç”¨ get-schema-suggestions å·¥å…·ç²å–è©³ç´°æ”¹é€²å»ºè­°")
                
                return output
                
            except Exception as e:
                logger.error(f"Analyze schema usage failed: {e}")
                return [f"âŒ Schema ä½¿ç”¨åˆ†æå¤±æ•—: {str(e)}"]

        async def get_schema_suggestions(
            ctx: Context,
            data_samples: Annotated[list[dict], Field(description="ç”¨æ–¼ç”Ÿæˆå»ºè­°çš„æ•¸æ“šæ¨£æœ¬åˆ—è¡¨")],
        ) -> list[str]:
            """
            åŸºæ–¼æ•¸æ“šä½¿ç”¨æƒ…æ³ç²å– Schema æ”¹é€²å»ºè­°ã€‚
            """
            await ctx.debug(f"Getting schema suggestions based on {len(data_samples)} samples")
            
            try:
                result = await schema_api.get_schema_suggestions(data_samples)
                
                if "error" in result:
                    return [f"âŒ {result['error']}"]
                
                output = [
                    f"ğŸ’¡ **Schema æ”¹é€²å»ºè­°**",
                    f"ğŸ“Š åˆ†æåŸºç¤: {result['analysis_summary']['total_samples']} å€‹æ¨£æœ¬",
                    f"ğŸ“‹ Schema ç‰ˆæœ¬: {result['analysis_summary']['schema_version']}",
                    f"âœ… åˆè¦ç‡: {result['analysis_summary']['compliance_rate']:.1%}",
                    f"ğŸ” å»ºè­°æ•¸é‡: {result['suggestion_count']}",
                    "",
                    f"ğŸ“ {result['message']}"
                ]
                
                if result["suggestions"]:
                    output.append("")
                    output.append("ğŸ¯ **å…·é«”å»ºè­°:**")
                    
                    for i, suggestion in enumerate(result["suggestions"], 1):
                        priority_emoji = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(
                            suggestion.get("priority", "low"), "âšª"
                        )
                        
                        output.append(f"{i}. {priority_emoji} **{suggestion['type']}**: {suggestion['field_name']}")
                        output.append(f"   åŸå› : {suggestion['reason']}")
                        output.append("")
                else:
                    output.append("")
                    output.append("ğŸ‰ ç›®å‰ Schema è¨­è¨ˆè‰¯å¥½ï¼Œç„¡éœ€èª¿æ•´ï¼")
                
                return output
                
            except Exception as e:
                logger.error(f"Get schema suggestions failed: {e}")
                return [f"âŒ ç²å– Schema å»ºè­°å¤±æ•—: {str(e)}"]

        async def get_schema_evolution_history(
            ctx: Context,
        ) -> list[str]:
            """
            ç²å– Schema æ¼”é€²æ­·å²è¨˜éŒ„ã€‚
            """
            await ctx.debug("Getting schema evolution history")
            
            try:
                result = await schema_api.get_schema_evolution_history()
                
                if "error" in result:
                    return [f"âŒ {result['error']}"]
                
                summary = result.get("summary", {})
                
                output = [
                    f"ğŸ“š **Schema æ¼”é€²æ­·å²**",
                    f"ğŸ“Š ç¸½ç‰ˆæœ¬æ•¸: {result['total_versions']}",
                    f"âœ… æ´»èºç‰ˆæœ¬æ•¸: {result['active_versions']}",
                    f"ğŸ”„ ç¸½é·ç§»æ•¸: {result['total_migrations']}",
                    "",
                    f"ğŸ é¦–å€‹ç‰ˆæœ¬: {summary.get('first_version', 'N/A')}",
                    f"ğŸš€ æœ€æ–°ç‰ˆæœ¬: {summary.get('latest_version', 'N/A')}",
                    f"ğŸ”¥ è®Šæ›´æœ€å¤šç‰ˆæœ¬: {summary.get('most_changes', 'N/A')}",
                    "",
                    f"ğŸ“‹ **ç‰ˆæœ¬è©³æƒ…:**"
                ]
                
                for version_info in result["evolution_history"]:
                    status = "âœ… æ´»èº" if version_info["is_active"] else "â¸ï¸ éæ´»èº"
                    compat = "ğŸ”— å…¼å®¹" if version_info["backward_compatible"] else "âš ï¸ ç ´å£æ€§"
                    
                    output.append(f"**ç‰ˆæœ¬ {version_info['version']}** {status} {compat}")
                    output.append(f"  - æè¿°: {version_info['description']}")
                    output.append(f"  - å»ºç«‹æ™‚é–“: {version_info['created_at']}")
                    output.append(f"  - æ¬„ä½æ•¸é‡: {version_info['field_count']}")
                    
                    if version_info["migrations"]:
                        output.append(f"  - é·ç§»è¨˜éŒ„:")
                        for migration in version_info["migrations"]:
                            output.append(f"    * {migration['type']}: {migration['field']} (ä¾†è‡ª {migration['from_version']})")
                    
                    output.append("")
                
                return output
                
            except Exception as e:
                logger.error(f"Get schema evolution history failed: {e}")
                return [f"âŒ ç²å– Schema æ¼”é€²æ­·å²å¤±æ•—: {str(e)}"]

        # Schema å¯©æŸ¥ç®¡ç†å·¥å…·
        async def list_pending_schema_requests(
            ctx: Context,
            reviewer: Annotated[str, Field(description="å¯©æŸ¥è€…èº«ä»½ï¼ˆç”¨æ–¼æ¬Šé™éæ¿¾ï¼‰")] = "admin",
        ) -> list[str]:
            """
            åˆ—å‡ºæ‰€æœ‰å¾…å¯©æŸ¥çš„ Schema è®Šæ›´è«‹æ±‚ã€‚
            """
            await ctx.debug(f"Listing pending schema requests for reviewer: {reviewer}")
            
            try:
                approval_manager = get_approval_manager()
                pending_requests = approval_manager.get_pending_requests(reviewer)
                
                if not pending_requests:
                    return [
                        "âœ… **ç›®å‰æ²’æœ‰å¾…å¯©æŸ¥çš„ Schema è®Šæ›´è«‹æ±‚**",
                        "ğŸ‰ æ‰€æœ‰è®Šæ›´è«‹æ±‚éƒ½å·²è™•ç†å®Œæˆ"
                    ]
                
                output = [
                    f"ğŸ“‹ **å¾…å¯©æŸ¥çš„ Schema è®Šæ›´è«‹æ±‚ ({len(pending_requests)} å€‹)**",
                    ""
                ]
                
                for i, request in enumerate(pending_requests, 1):
                    risk_emoji = {
                        "low": "ğŸŸ¢",
                        "medium": "ğŸŸ¡", 
                        "high": "ğŸ”´",
                        "critical": "ğŸš¨"
                    }.get(request["risk_level"], "âšª")
                    
                    output.extend([
                        f"**{i}. è«‹æ±‚ {request['request_id'][:8]}...** {risk_emoji}",
                        f"   ğŸ”§ è®Šæ›´é¡å‹: {request['change_type']}",
                        f"   ğŸ—ï¸ æ¬„ä½åç¨±: {request['field_name']}",
                        f"   âš ï¸ é¢¨éšªç´šåˆ¥: {request['risk_level']}",
                        f"   ğŸ‘¥ éœ€è¦æ¬Šé™: {request['required_approval_level']}",
                        f"   ğŸ‘¤ ææ¡ˆè€…: {request['proposed_by']}",
                        f"   ğŸ“… ææ¡ˆæ™‚é–“: {request['proposed_at'][:19]}",
                        f"   ğŸ“ ç†ç”±: {request['justification'][:100]}{'...' if len(request['justification']) > 100 else ''}",
                        ""
                    ])
                
                output.extend([
                    "ğŸ’¡ **å¯©æŸ¥æŒ‡ä»¤:**",
                    "âœ… æ ¸å‡†: review-schema-request <request_id> approve <reviewer> [comments]",
                    "âŒ æ‹’çµ•: review-schema-request <request_id> reject <reviewer> [comments]"
                ])
                
                return output
                
            except Exception as e:
                logger.error(f"List pending schema requests failed: {e}")
                return [f"âŒ åˆ—å‡ºå¾…å¯©æŸ¥è«‹æ±‚å¤±æ•—: {str(e)}"]

        async def review_schema_request(
            ctx: Context,
            request_id: Annotated[str, Field(description="è¦å¯©æŸ¥çš„è«‹æ±‚ID")],
            action: Annotated[str, Field(description="å¯©æŸ¥å‹•ä½œ: approve æˆ– reject")],
            reviewer: Annotated[str, Field(description="å¯©æŸ¥è€…èº«ä»½")],
            comments: Annotated[str, Field(description="å¯©æŸ¥æ„è¦‹")] = "",
        ) -> list[str]:
            """
            å¯©æŸ¥ Schema è®Šæ›´è«‹æ±‚ï¼Œé€²è¡Œæ ¸å‡†æˆ–æ‹’çµ•ã€‚
            """
            await ctx.debug(f"Reviewing schema request {request_id}: {action} by {reviewer}")
            
            try:
                if action not in ["approve", "reject"]:
                    return [f"âŒ ç„¡æ•ˆçš„å¯©æŸ¥å‹•ä½œ: {action}ï¼Œè«‹ä½¿ç”¨ 'approve' æˆ– 'reject'"]
                
                approval_manager = get_approval_manager()
                
                # æª¢æŸ¥è«‹æ±‚æ˜¯å¦å­˜åœ¨
                if request_id not in approval_manager.pending_requests:
                    return [
                        f"âŒ è«‹æ±‚ä¸å­˜åœ¨: {request_id}",
                        "ğŸ” ä½¿ç”¨ 'list-pending-schema-requests' æŸ¥çœ‹æ‰€æœ‰å¾…å¯©æŸ¥è«‹æ±‚"
                    ]
                
                # åŸ·è¡Œå¯©æŸ¥
                success = approval_manager.review_request(
                    request_id=request_id,
                    reviewer=reviewer,
                    action=action,
                    comments=comments
                )
                
                if not success:
                    return [
                        f"âŒ å¯©æŸ¥å¤±æ•—: æ¬Šé™ä¸è¶³æˆ–è«‹æ±‚ä¸å­˜åœ¨",
                        f"ğŸ‘¤ å¯©æŸ¥è€…: {reviewer}",
                        f"ğŸ“‹ è«‹æ±‚ID: {request_id}",
                        "ğŸ’¡ è«‹æª¢æŸ¥æ‚¨çš„å¯©æŸ¥æ¬Šé™"
                    ]
                
                # å¾æ­·å²è¨˜éŒ„ä¸­æ‰¾åˆ°å¯©æŸ¥çµæœ
                reviewed_request = next(
                    req for req in approval_manager.approval_history
                    if req.request_id == request_id
                )
                
                if action == "approve":
                    if reviewed_request.status == "approved":
                        return [
                            f"âœ… **Schema è®Šæ›´è«‹æ±‚å·²æ ¸å‡†ä¸¦åŸ·è¡Œ**",
                            f"ğŸ“‹ è«‹æ±‚ID: {request_id}",
                            f"ğŸ—ï¸ è®Šæ›´é¡å‹: {reviewed_request.change_type}",
                            f"ğŸ”§ æ¬„ä½: {reviewed_request.field_name}",
                            f"ğŸ‘¤ å¯©æŸ¥è€…: {reviewer}",
                            f"ğŸ“ å¯©æŸ¥æ„è¦‹: {comments}",
                            f"â° å¯©æŸ¥æ™‚é–“: {reviewed_request.reviewed_at.strftime('%Y-%m-%d %H:%M:%S')}",
                            "",
                            f"ğŸ‰ Schema è®Šæ›´å·²æˆåŠŸæ‡‰ç”¨ï¼"
                        ]
                    else:
                        return [
                            f"âŒ **Schema è®Šæ›´åŸ·è¡Œå¤±æ•—**",
                            f"ğŸ“‹ è«‹æ±‚ID: {request_id}",
                            f"ğŸ‘¤ å¯©æŸ¥è€…: {reviewer}",
                            f"ğŸ’¬ éŒ¯èª¤: {reviewed_request.review_comments}",
                            "ğŸ”§ è«‹æª¢æŸ¥ Schema å®šç¾©æ˜¯å¦æ­£ç¢º"
                        ]
                else:  # reject
                    return [
                        f"âŒ **Schema è®Šæ›´è«‹æ±‚å·²æ‹’çµ•**",
                        f"ğŸ“‹ è«‹æ±‚ID: {request_id}",
                        f"ğŸ—ï¸ è®Šæ›´é¡å‹: {reviewed_request.change_type}",
                        f"ğŸ”§ æ¬„ä½: {reviewed_request.field_name}",
                        f"ğŸ‘¤ å¯©æŸ¥è€…: {reviewer}",
                        f"ğŸ“ æ‹’çµ•ç†ç”±: {comments}",
                        f"â° å¯©æŸ¥æ™‚é–“: {reviewed_request.reviewed_at.strftime('%Y-%m-%d %H:%M:%S')}"
                    ]
                
            except Exception as e:
                logger.error(f"Review schema request failed: {e}")
                return [f"âŒ å¯©æŸ¥ Schema è«‹æ±‚å¤±æ•—: {str(e)}"]

        async def get_schema_approval_history(
            ctx: Context,
            limit: Annotated[int, Field(description="è¿”å›çš„æ­·å²è¨˜éŒ„æ•¸é‡")] = 10,
        ) -> list[str]:
            """
            ç²å– Schema å¯©æŸ¥æ­·å²è¨˜éŒ„ã€‚
            """
            await ctx.debug(f"Getting schema approval history (limit: {limit})")
            
            try:
                approval_manager = get_approval_manager()
                history = approval_manager.get_approval_history(limit)
                
                if not history:
                    return [
                        "ğŸ“‹ **æš«ç„¡ Schema å¯©æŸ¥æ­·å²è¨˜éŒ„**",
                        "ğŸ’¡ ç•¶æœ‰ Schema è®Šæ›´è«‹æ±‚æ™‚ï¼Œè¨˜éŒ„æœƒé¡¯ç¤ºåœ¨é€™è£¡"
                    ]
                
                output = [
                    f"ğŸ“š **Schema å¯©æŸ¥æ­·å²è¨˜éŒ„ (æœ€è¿‘ {len(history)} å€‹)**",
                    ""
                ]
                
                for i, record in enumerate(history, 1):
                    status_emoji = {"approved": "âœ…", "rejected": "âŒ"}.get(record["status"], "â³")
                    risk_emoji = {
                        "low": "ğŸŸ¢",
                        "medium": "ğŸŸ¡",
                        "high": "ğŸ”´", 
                        "critical": "ğŸš¨"
                    }.get(record["risk_level"], "âšª")
                    
                    output.extend([
                        f"**{i}. è«‹æ±‚ {record['request_id'][:8]}...** {status_emoji} {risk_emoji}",
                        f"   ğŸ”§ è®Šæ›´: {record['change_type']} â†’ {record['field_name']}",
                        f"   ğŸ‘¤ ææ¡ˆè€…: {record['proposed_by']}",
                        f"   ğŸ‘¥ å¯©æŸ¥è€…: {record['reviewed_by'] or 'N/A'}",
                        f"   ğŸ“… ææ¡ˆæ™‚é–“: {record['proposed_at'][:19]}",
                        f"   â° å¯©æŸ¥æ™‚é–“: {record['reviewed_at'][:19] if record['reviewed_at'] else 'N/A'}",
                        f"   ğŸ“ å¯©æŸ¥æ„è¦‹: {record['review_comments'][:80]}{'...' if len(record['review_comments']) > 80 else ''}",
                        ""
                    ])
                
                return output
                
            except Exception as e:
                logger.error(f"Get schema approval history failed: {e}")
                return [f"âŒ ç²å–å¯©æŸ¥æ­·å²å¤±æ•—: {str(e)}"]

        # è¨»å†Š Schema ç®¡ç†å·¥å…·
        self.tool(
            get_current_schema,
            name="get-current-schema",
            description="ç²å–ç•¶å‰æ´»èºçš„ Schema ç‰ˆæœ¬åŠå…¶è©³ç´°è³‡è¨Š",
        )
        
        self.tool(
            validate_schema_data,
            name="validate-schema-data", 
            description="é©—è­‰æ•¸æ“šæ˜¯å¦ç¬¦åˆ Schema è¦ç¯„",
        )
        
        self.tool(
            analyze_schema_usage,
            name="analyze-schema-usage",
            description="åˆ†æ Schema ä½¿ç”¨æƒ…æ³ï¼Œæä¾›çµ±è¨ˆè³‡è¨Š",
        )
        
        self.tool(
            get_schema_suggestions,
            name="get-schema-suggestions",
            description="åŸºæ–¼æ•¸æ“šä½¿ç”¨æƒ…æ³ç²å– Schema æ”¹é€²å»ºè­°",
        )
        
        self.tool(
            get_schema_evolution_history,
            name="get-schema-evolution-history",
            description="ç²å– Schema æ¼”é€²æ­·å²è¨˜éŒ„",
        )
        
        self.tool(
            list_pending_schema_requests,
            name="list-pending-schema-requests",
            description="åˆ—å‡ºæ‰€æœ‰å¾…å¯©æŸ¥çš„ Schema è®Šæ›´è«‹æ±‚",
        )
        
        self.tool(
            review_schema_request,
            name="review-schema-request",
            description="å¯©æŸ¥ Schema è®Šæ›´è«‹æ±‚ï¼Œé€²è¡Œæ ¸å‡†æˆ–æ‹’çµ•",
        )
        
        self.tool(
            get_schema_approval_history,
            name="get-schema-approval-history",
            description="ç²å– Schema å¯©æŸ¥æ­·å²è¨˜éŒ„",
        )
        
        # åªåœ¨éå”¯è®€æ¨¡å¼ä¸‹è¨»å†Šè®Šæ›´è«‹æ±‚å·¥å…·
        if not self.qdrant_settings.read_only:
            self.tool(
                request_schema_field_addition,
                name="request-schema-field-addition",
                description="è«‹æ±‚æ–°å¢ Schema æ¬„ä½ï¼Œå°‡å‰µå»ºå¯©æŸ¥è«‹æ±‚è€Œéç›´æ¥åŸ·è¡Œè®Šæ›´",
            )
            
            self.tool(
                request_schema_field_removal,
                name="request-schema-field-removal",
                description="è«‹æ±‚ç§»é™¤ Schema æ¬„ä½ï¼Œå°‡å‰µå»ºå¯©æŸ¥è«‹æ±‚ï¼ˆé«˜é¢¨éšªæ“ä½œï¼‰",
            )

        # æ¬Šé™ç®¡ç†å·¥å…·é›† - æ‰€æœ‰ç”¨æˆ¶éƒ½å¯ä»¥æŸ¥çœ‹æ¬Šé™ç‹€æ…‹
        async def get_user_permissions(ctx: Context) -> list[str]:
            """
            ç²å–ç•¶å‰ç”¨æˆ¶çš„æ¬Šé™æ‘˜è¦å’Œå¯ç”¨å·¥å…·åˆ—è¡¨ã€‚
            """
            await ctx.debug("Getting user permissions")
            
            try:
                user_id = "default_user"  # ç›®å‰ä½¿ç”¨é è¨­ç”¨æˆ¶
                summary = self.permission_manager.get_permission_summary(user_id)
                
                output = [
                    f"ğŸ‘¤ **ç”¨æˆ¶æ¬Šé™è³‡è¨Š**",
                    f"ğŸ†” ç”¨æˆ¶ID: {summary['user_id']}",
                    f"ğŸ” æ¬Šé™ç´šåˆ¥: {summary['permission_level']}",
                    f"ğŸ› ï¸ å¯ç”¨å·¥å…·ç¸½æ•¸: {summary['total_available_tools']}",
                    "",
                    f"ğŸ¯ **å¯åŸ·è¡Œæ“ä½œé¡å‹:**"
                ]
                
                for operation in summary['available_operations']:
                    output.append(f"  âœ… {operation}")
                
                output.append("")
                output.append(f"ğŸ› ï¸ **æŒ‰é¢¨éšªç´šåˆ¥åˆ†é¡çš„å¯ç”¨å·¥å…·:**")
                
                for risk_level, tools in summary['tools_by_risk'].items():
                    if tools:
                        risk_emoji = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "critical": "ğŸ”´"}.get(risk_level, "âšª")
                        output.append(f"  {risk_emoji} **{risk_level.upper()} é¢¨éšª ({len(tools)} å€‹):**")
                        for tool in sorted(tools):
                            output.append(f"    - {tool}")
                        output.append("")
                
                if summary['permission_level'] == 'user':
                    output.append("ğŸ’¡ **æå‡æ¬Šé™:**")
                    output.append("å¦‚éœ€ä½¿ç”¨ç®¡ç†åŠŸèƒ½ï¼Œè«‹è¯çµ¡ç®¡ç†å“¡æå‡æ¬Šé™ç´šåˆ¥è‡³ admin æˆ– super_admin")
                
                return output
                
            except Exception as e:
                logger.error(f"Get user permissions failed: {e}")
                return [f"âŒ ç²å–æ¬Šé™è³‡è¨Šå¤±æ•—: {str(e)}"]

        async def check_tool_permission(
            ctx: Context,
            tool_name: Annotated[str, Field(description="è¦æª¢æŸ¥çš„å·¥å…·åç¨±")],
        ) -> list[str]:
            """
            æª¢æŸ¥ç‰¹å®šå·¥å…·çš„ä½¿ç”¨æ¬Šé™ã€‚
            """
            await ctx.debug(f"Checking permission for tool: {tool_name}")
            
            try:
                user_id = "default_user"
                has_permission = self.permission_manager.check_tool_permission(user_id, tool_name)
                user_level = self.permission_manager.get_user_permission(user_id)
                tool_permission = self.permission_manager.tool_permissions.get(tool_name)
                
                if not tool_permission:
                    return [
                        f"â“ **å·¥å…·æ¬Šé™æŸ¥è©¢**",
                        f"ğŸ”§ å·¥å…·åç¨±: {tool_name}",
                        f"âš ï¸ ç‹€æ…‹: æœªå®šç¾©çš„å·¥å…·",
                        f"ğŸ’¬ èªªæ˜: æ­¤å·¥å…·å¯èƒ½ä¸å­˜åœ¨æˆ–æœªåœ¨æ¬Šé™ç³»çµ±ä¸­è¨»å†Š"
                    ]
                
                permission_emoji = "âœ…" if has_permission else "âŒ"
                risk_emoji = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸ”´", "critical": "ğŸš¨"}.get(
                    tool_permission.risk_level, "âšª"
                )
                
                output = [
                    f"ğŸ” **å·¥å…·æ¬Šé™æª¢æŸ¥çµæœ**",
                    f"ğŸ”§ å·¥å…·åç¨±: {tool_name}",
                    f"{permission_emoji} ä½¿ç”¨æ¬Šé™: {'å…è¨±' if has_permission else 'æ‹’çµ•'}",
                    f"ğŸ‘¤ ç•¶å‰æ¬Šé™ç´šåˆ¥: {user_level.value}",
                    f"âš ï¸ éœ€è¦æ¬Šé™ç´šåˆ¥: {tool_permission.required_level.value}",
                    f"{risk_emoji} é¢¨éšªç´šåˆ¥: {tool_permission.risk_level}",
                    f"ğŸ“ å·¥å…·èªªæ˜: {tool_permission.description}",
                ]
                
                if not has_permission:
                    output.append("")
                    output.append("ğŸ’¡ **è§£æ±ºæ–¹æ¡ˆ:**")
                    if tool_permission.required_level == PermissionLevel.ADMIN:
                        output.append("éœ€è¦ admin æ¬Šé™ï¼Œè«‹è¯çµ¡ç®¡ç†å“¡æå‡æ¬Šé™ç´šåˆ¥")
                    elif tool_permission.required_level == PermissionLevel.SUPER_ADMIN:
                        output.append("éœ€è¦ super_admin æ¬Šé™ï¼Œæ­¤ç‚ºé«˜é¢¨éšªæ“ä½œï¼Œéœ€è¦æœ€é«˜ç®¡ç†å“¡æ¬Šé™")
                
                return output
                
            except Exception as e:
                logger.error(f"Check tool permission failed: {e}")
                return [f"âŒ æª¢æŸ¥å·¥å…·æ¬Šé™å¤±æ•—: {str(e)}"]

        # è¨»å†Šæ¬Šé™ç®¡ç†å·¥å…·
        self.tool(
            get_user_permissions,
            name="get-user-permissions",
            description="ç²å–ç•¶å‰ç”¨æˆ¶çš„æ¬Šé™æ‘˜è¦å’Œå¯ç”¨å·¥å…·åˆ—è¡¨",
        )
        
        self.tool(
            check_tool_permission,
            name="check-tool-permission",
            description="æª¢æŸ¥ç‰¹å®šå·¥å…·çš„ä½¿ç”¨æ¬Šé™",
        )

        # åªæœ‰ super_admin å¯ä»¥ç®¡ç†æ¬Šé™
        async def set_user_permission_level(
            ctx: Context,
            user_id: Annotated[str, Field(description="è¦è¨­å®šçš„ç”¨æˆ¶ID")],
            permission_level: Annotated[str, Field(description="æ¬Šé™ç´šåˆ¥: user, admin, super_admin")],
        ) -> str:
            """
            è¨­å®šç”¨æˆ¶çš„æ¬Šé™ç´šåˆ¥ï¼ˆåƒ…é™è¶…ç´šç®¡ç†å“¡ï¼‰ã€‚
            """
            await ctx.debug(f"Setting user {user_id} permission to {permission_level}")
            
            try:
                # æª¢æŸ¥ç•¶å‰ç”¨æˆ¶æ˜¯å¦æœ‰æ¬Šé™åŸ·è¡Œæ­¤æ“ä½œ
                current_user = "default_user"
                current_level = self.permission_manager.get_user_permission(current_user)
                
                if current_level != PermissionLevel.SUPER_ADMIN:
                    return f"âŒ æ¬Šé™ä¸è¶³ï¼šåªæœ‰ super_admin å¯ä»¥ç®¡ç†ç”¨æˆ¶æ¬Šé™ï¼ˆç•¶å‰ç´šåˆ¥: {current_level.value}ï¼‰"
                
                # é©—è­‰æ¬Šé™ç´šåˆ¥
                try:
                    new_level = PermissionLevel(permission_level)
                except ValueError:
                    return f"âŒ ç„¡æ•ˆçš„æ¬Šé™ç´šåˆ¥: {permission_level}ï¼Œè«‹ä½¿ç”¨: user, admin, super_admin"
                
                # è¨­å®šæ¬Šé™
                self.permission_manager.set_user_permission(user_id, new_level)
                
                return f"âœ… æˆåŠŸè¨­å®šç”¨æˆ¶ {user_id} çš„æ¬Šé™ç´šåˆ¥ç‚º {new_level.value}"
                
            except Exception as e:
                logger.error(f"Set user permission failed: {e}")
                return f"âŒ è¨­å®šç”¨æˆ¶æ¬Šé™å¤±æ•—: {str(e)}"

        # åªåœ¨å•Ÿç”¨æ¬Šé™ç³»çµ±ä¸”ç‚ºè¶…ç´šç®¡ç†å“¡æ™‚è¨»å†Š
        if (self.qdrant_settings.enable_permission_system and 
            self.permission_manager.get_user_permission("default_user") == PermissionLevel.SUPER_ADMIN):
            self.tool(
                set_user_permission_level,
                name="set-user-permission-level",
                description="è¨­å®šç”¨æˆ¶çš„æ¬Šé™ç´šåˆ¥ï¼ˆåƒ…é™è¶…ç´šç®¡ç†å“¡ï¼‰",
            )

        # è³‡æ–™é·ç§»å·¥å…·é›† - éœ€è¦ç®¡ç†å“¡æ¬Šé™
        async def analyze_collection_for_migration(
            ctx: Context,
            collection_name: Annotated[str, Field(description="è¦åˆ†æçš„ collection åç¨±")],
        ) -> list[str]:
            """
            åˆ†æèˆŠ collection çš„çµæ§‹ï¼Œç‚ºé·ç§»åšæº–å‚™ã€‚
            """
            await ctx.debug(f"Analyzing collection for migration: {collection_name}")
            
            try:
                analysis = self.migration_tool.analyze_collection_structure(collection_name)
                
                output = [
                    f"ğŸ“Š **Collection åˆ†æçµæœ**",
                    f"ğŸ—ï¸ Collection: {analysis['collection_name']}",
                    f"ğŸ“¦ ç¸½é»æ•¸: {analysis['total_points']:,}",
                    f"ğŸ”¢ å‘é‡ç¶­åº¦: {analysis['vector_size']}",
                    f"ğŸ“ è·é›¢åº¦é‡: {analysis['distance_metric']}",
                    f"ğŸ“… åˆ†ææ™‚é–“: {analysis['analyzed_at']}",
                    "",
                    f"ğŸ—ƒï¸ **æ¬„ä½çµæ§‹åˆ†æ:**"
                ]
                
                for field, types in analysis['field_types'].items():
                    types_str = ', '.join(types)
                    samples = analysis['field_samples'].get(field, [])
                    sample_preview = str(samples[0]) if samples else "N/A"
                    if len(sample_preview) > 50:
                        sample_preview = sample_preview[:47] + "..."
                    
                    output.append(f"  ğŸ“ **{field}** ({types_str})")
                    output.append(f"    ç¯„ä¾‹: {sample_preview}")
                
                # ç”Ÿæˆå»ºè­°çš„é·ç§»è¨ˆåŠƒ
                suggested_plan = self.migration_tool.suggest_migration_plan(analysis)
                
                output.extend([
                    "",
                    f"ğŸ’¡ **å»ºè­°çš„é·ç§»è¨ˆåŠƒ:**",
                    f"ğŸ¯ ç›®æ¨™å…§å®¹é¡å‹: {suggested_plan.target_content_type.value}",
                    f"ğŸ“‹ é ä¼°è¨˜éŒ„æ•¸: {suggested_plan.estimated_records:,}",
                    "",
                    f"ğŸ”„ **æ¬„ä½æ˜ å°„å»ºè­°:**"
                ])
                
                for old_field, new_field in suggested_plan.mapping_rules.items():
                    output.append(f"  {old_field} â†’ {new_field}")
                
                output.extend([
                    "",
                    f"âš™ï¸ **è½‰æ›è¦å‰‡:**",
                    f"  æ¨™æº–åŒ–è©å½™: {'âœ…' if suggested_plan.transformation_rules.get('standardize_vocabulary') else 'âŒ'}",
                    f"  æå–é—œéµè©: {'âœ…' if suggested_plan.transformation_rules.get('extract_keywords') else 'âŒ'}",
                    f"  æ­£è¦åŒ–æ¨™ç±¤: {'âœ…' if suggested_plan.transformation_rules.get('normalize_tags') else 'âŒ'}",
                    "",
                    f"ğŸ’¡ **ä¸‹ä¸€æ­¥:** ä½¿ç”¨ 'create-migration-plan' å·¥å…·å‰µå»ºæ­£å¼çš„é·ç§»è¨ˆåŠƒ"
                ])
                
                return output
                
            except Exception as e:
                logger.error(f"Collection analysis failed: {e}")
                return [f"âŒ Collection åˆ†æå¤±æ•—: {str(e)}"]

        async def execute_migration_dry_run(
            ctx: Context,
            source_collection: Annotated[str, Field(description="ä¾†æº collection åç¨±")],
            target_content_type: Annotated[str, Field(description="ç›®æ¨™å…§å®¹é¡å‹: experience, process_workflow, knowledge_base, decision_record, vocabulary")],
            batch_size: Annotated[int, Field(description="æ‰¹æ¬¡å¤§å°")] = 100,
        ) -> list[str]:
            """
            åŸ·è¡Œé·ç§»é æ¼”ï¼ˆä¸å¯¦éš›ç§»å‹•è³‡æ–™ï¼‰ï¼Œæª¢æŸ¥é·ç§»å¯è¡Œæ€§ã€‚
            """
            await ctx.debug(f"Running migration dry run: {source_collection} -> {target_content_type}")
            
            try:
                # å‰µå»ºé·ç§»è¨ˆåŠƒ
                analysis = self.migration_tool.analyze_collection_structure(source_collection)
                plan = self.migration_tool.suggest_migration_plan(analysis)
                
                # æ›´æ–°ç›®æ¨™å…§å®¹é¡å‹
                from mcp_server_qdrant.ragbridge.models import ContentType
                try:
                    plan.target_content_type = ContentType(target_content_type)
                except ValueError:
                    return [f"âŒ ç„¡æ•ˆçš„å…§å®¹é¡å‹: {target_content_type}"]
                
                # åŸ·è¡Œ dry run
                result = await self.migration_tool.execute_migration(
                    plan=plan,
                    dry_run=True,
                    batch_size=batch_size
                )
                
                output = [
                    f"ğŸ§ª **é·ç§»é æ¼”çµæœ**",
                    f"ğŸ“‹ ä¾†æº: {result.plan.source_collection}",
                    f"ğŸ¯ ç›®æ¨™: {result.plan.target_content_type.value}",
                    f"â±ï¸ åŸ·è¡Œæ™‚é–“: {result.duration_seconds:.1f} ç§’",
                    "",
                    f"ğŸ“Š **è™•ç†çµ±è¨ˆ:**",
                    f"  ç¸½è¨˜éŒ„æ•¸: {result.total_records:,}",
                    f"  æˆåŠŸè™•ç†: {result.successful_records:,}",
                    f"  è™•ç†å¤±æ•—: {result.failed_records:,}",
                    f"  æˆåŠŸç‡: {result.success_rate:.1%}",
                ]
                
                if result.errors:
                    output.extend([
                        "",
                        f"âš ï¸ **ç™¼ç¾çš„å•é¡Œ (å‰10å€‹):**"
                    ])
                    for error in result.errors[:10]:
                        output.append(f"  â€¢ {error}")
                    
                    if len(result.errors) > 10:
                        output.append(f"  ... é‚„æœ‰ {len(result.errors) - 10} å€‹éŒ¯èª¤")
                
                # ç”Ÿæˆå»ºè­°
                report = self.migration_tool.generate_migration_report(result)
                if report['recommendations']:
                    output.extend([
                        "",
                        f"ğŸ’¡ **å»ºè­°:**"
                    ])
                    for rec in report['recommendations']:
                        output.append(f"  â€¢ {rec}")
                
                if result.success_rate >= 0.9:
                    output.extend([
                        "",
                        f"âœ… **é æ¼”æˆåŠŸï¼** å¯ä»¥ä½¿ç”¨ 'execute-migration' å·¥å…·åŸ·è¡Œå¯¦éš›é·ç§»"
                    ])
                else:
                    output.extend([
                        "",
                        f"âš ï¸ **é æ¼”ç™¼ç¾å•é¡Œï¼** å»ºè­°å…ˆä¿®æ­£å•é¡Œå†åŸ·è¡Œå¯¦éš›é·ç§»"
                    ])
                
                return output
                
            except Exception as e:
                logger.error(f"Migration dry run failed: {e}")
                return [f"âŒ é·ç§»é æ¼”å¤±æ•—: {str(e)}"]

        async def execute_data_migration(
            ctx: Context,
            source_collection: Annotated[str, Field(description="ä¾†æº collection åç¨±")],
            target_content_type: Annotated[str, Field(description="ç›®æ¨™å…§å®¹é¡å‹")],
            create_backup: Annotated[bool, Field(description="æ˜¯å¦å‰µå»ºå‚™ä»½")] = True,
            batch_size: Annotated[int, Field(description="æ‰¹æ¬¡å¤§å°")] = 100,
        ) -> list[str]:
            """
            åŸ·è¡Œå¯¦éš›çš„è³‡æ–™é·ç§»ï¼ˆé«˜é¢¨éšªæ“ä½œï¼Œéœ€è¦ super_admin æ¬Šé™ï¼‰ã€‚
            """
            await ctx.debug(f"Executing data migration: {source_collection} -> {target_content_type}")
            
            try:
                # å‰µå»ºé·ç§»è¨ˆåŠƒ
                analysis = self.migration_tool.analyze_collection_structure(source_collection)
                plan = self.migration_tool.suggest_migration_plan(analysis)
                
                # æ›´æ–°ç›®æ¨™å…§å®¹é¡å‹
                from mcp_server_qdrant.ragbridge.models import ContentType
                try:
                    plan.target_content_type = ContentType(target_content_type)
                except ValueError:
                    return [f"âŒ ç„¡æ•ˆçš„å…§å®¹é¡å‹: {target_content_type}"]
                
                # é©—è­‰è¨ˆåŠƒ
                validation_errors = self.migration_tool.validate_migration_plan(plan)
                if validation_errors:
                    return [
                        f"âŒ **é·ç§»è¨ˆåŠƒé©—è­‰å¤±æ•—:**",
                        *[f"  â€¢ {error}" for error in validation_errors]
                    ]
                
                # åŸ·è¡Œé·ç§»
                result = await self.migration_tool.execute_migration(
                    plan=plan,
                    dry_run=False,
                    batch_size=batch_size
                )
                
                output = [
                    f"ğŸš€ **è³‡æ–™é·ç§»åŸ·è¡Œçµæœ**",
                    f"ğŸ“‹ ä¾†æº: {result.plan.source_collection}",
                    f"ğŸ¯ ç›®æ¨™: {result.plan.target_content_type.value}",
                    f"â±ï¸ åŸ·è¡Œæ™‚é–“: {result.duration_seconds:.1f} ç§’",
                    "",
                    f"ğŸ“Š **é·ç§»çµ±è¨ˆ:**",
                    f"  ç¸½è¨˜éŒ„æ•¸: {result.total_records:,}",
                    f"  æˆåŠŸé·ç§»: {result.successful_records:,}",
                    f"  é·ç§»å¤±æ•—: {result.failed_records:,}",
                    f"  æˆåŠŸç‡: {result.success_rate:.1%}",
                ]
                
                if create_backup:
                    output.append(f"ğŸ’¾ å‚™ä»½å·²å‰µå»º")
                
                if result.errors:
                    output.extend([
                        "",
                        f"âš ï¸ **é·ç§»éŒ¯èª¤ (å‰10å€‹):**"
                    ])
                    for error in result.errors[:10]:
                        output.append(f"  â€¢ {error}")
                
                # ç”Ÿæˆæœ€çµ‚å»ºè­°
                if result.success_rate >= 0.95:
                    output.extend([
                        "",
                        f"âœ… **é·ç§»æˆåŠŸå®Œæˆï¼**",
                        f"ğŸ’¡ å»ºè­°ä½¿ç”¨ 'qdrant-list-collections' æª¢æŸ¥æ–°çš„ collection",
                        f"âš ï¸ å¦‚æœç¢ºèªé·ç§»æˆåŠŸï¼Œå¯ä»¥è€ƒæ…®ç§»é™¤åŸå§‹ collection"
                    ])
                elif result.success_rate >= 0.8:
                    output.extend([
                        "",
                        f"âš ï¸ **é·ç§»éƒ¨åˆ†æˆåŠŸ**",
                        f"ğŸ’¡ å»ºè­°æª¢æŸ¥å¤±æ•—çš„è¨˜éŒ„ä¸¦è€ƒæ…®é‡æ–°é·ç§»"
                    ])
                else:
                    output.extend([
                        "",
                        f"âŒ **é·ç§»å¤±æ•—ç‡éé«˜**",
                        f"ğŸ’¡ å»ºè­°æª¢æŸ¥éŒ¯èª¤åŸå› ä¸¦èª¿æ•´é·ç§»ç­–ç•¥"
                    ])
                
                return output
                
            except Exception as e:
                logger.error(f"Data migration failed: {e}")
                return [f"âŒ è³‡æ–™é·ç§»å¤±æ•—: {str(e)}"]

        # è¨»å†Šé·ç§»å·¥å…·ï¼ˆéœ€è¦ç®¡ç†å“¡æ¬Šé™ï¼‰
        if not self.qdrant_settings.read_only:
            self.tool(
                analyze_collection_for_migration,
                name="analyze-collection-for-migration",
                description="åˆ†æèˆŠ collection çš„çµæ§‹ï¼Œç‚ºé·ç§»åšæº–å‚™",
            )
            
            self.tool(
                execute_migration_dry_run,
                name="execute-migration-dry-run",
                description="åŸ·è¡Œé·ç§»é æ¼”ï¼ˆä¸å¯¦éš›ç§»å‹•è³‡æ–™ï¼‰ï¼Œæª¢æŸ¥é·ç§»å¯è¡Œæ€§",
            )
            
            # å¯¦éš›é·ç§»å·¥å…·åªåœ¨éå”¯è®€æ¨¡å¼ä¸‹æä¾›
            self.tool(
                execute_data_migration,
                name="execute-data-migration",
                description="åŸ·è¡Œå¯¦éš›çš„è³‡æ–™é·ç§»ï¼ˆé«˜é¢¨éšªæ“ä½œï¼Œéœ€è¦ç®¡ç†å“¡æ¬Šé™ï¼‰",
            )

        # ç’°å¢ƒè®Šæ•¸æª¢æŸ¥å·¥å…·
        async def check_environment_config(ctx: Context) -> list[str]:
            """
            æª¢æŸ¥ç³»çµ±çš„ç’°å¢ƒè®Šæ•¸é…ç½®ï¼Œç”¨æ–¼èª¿è©¦å’Œé©—è­‰è¨­ç½®ã€‚
            """
            import os
            from pathlib import Path
            
            result = ["ğŸ”§ **ç’°å¢ƒè®Šæ•¸é…ç½®æª¢æŸ¥**", ""]
            
            # æª¢æŸ¥ .env æ–‡ä»¶è·¯å¾‘
            package_dir = Path(__file__).parent.parent.parent
            env_path = package_dir / ".env"
            result.append(f"ğŸ“ **å°ˆæ¡ˆæ ¹ç›®éŒ„**: {package_dir}")
            result.append(f"ğŸ“„ **.env æ–‡ä»¶è·¯å¾‘**: {env_path}")
            result.append(f"âœ… **.env æ–‡ä»¶å­˜åœ¨**: {'æ˜¯' if env_path.exists() else 'å¦'}")
            result.append("")
            
            # åˆ—å‡ºæ‰€æœ‰ç›¸é—œçš„ç’°å¢ƒè®Šæ•¸
            env_vars = {
                "Qdrant é…ç½®": [
                    "QDRANT_URL",
                    "QDRANT_API_KEY", 
                    "QDRANT_LOCAL_PATH",
                    "COLLECTION_NAME",
                    "QDRANT_SEARCH_LIMIT",
                    "QDRANT_READ_ONLY",
                    "QDRANT_ALLOW_ARBITRARY_FILTER"
                ],
                "æ¬Šé™ç³»çµ±": [
                    "QDRANT_ENABLE_PERMISSION_SYSTEM",
                    "QDRANT_DEFAULT_PERMISSION_LEVEL"
                ],
                "Embedding é…ç½®": [
                    "EMBEDDING_PROVIDER",
                    "EMBEDDING_MODEL",
                    "OLLAMA_BASE_URL"
                ],
                "å·¥å…·é…ç½®": [
                    "TOOL_STORE_DESCRIPTION",
                    "TOOL_FIND_DESCRIPTION"
                ]
            }
            
            for category, vars_list in env_vars.items():
                result.append(f"ğŸ“‹ **{category}**:")
                for var in vars_list:
                    value = os.getenv(var)
                    if value is not None:
                        # å°æ–¼æ•æ„Ÿè³‡è¨Šï¼ˆå¦‚ API KEYï¼‰é€²è¡Œé®ç½©
                        if "API_KEY" in var or "TOKEN" in var:
                            display_value = f"{value[:8]}..." if len(value) > 8 else value
                        else:
                            display_value = value
                        result.append(f"   âœ… {var} = {display_value}")
                    else:
                        result.append(f"   âŒ {var} = (æœªè¨­ç½®)")
                result.append("")
            
            # æª¢æŸ¥ç•¶å‰è¨­ç½®ç‰©ä»¶çš„å¯¦éš›å€¼
            result.append("âš™ï¸ **ç•¶å‰è¨­å®šç‰©ä»¶å€¼**:")
            result.append(f"   ğŸ“ Qdrant URL: {self.qdrant_settings.location}")
            result.append(f"   ğŸ”‘ API Key: {'å·²è¨­ç½®' if self.qdrant_settings.api_key else 'æœªè¨­ç½®'}")
            result.append(f"   ğŸ“¦ Collection: {self.qdrant_settings.collection_name}")
            result.append(f"   ğŸ” Search Limit: {self.qdrant_settings.search_limit}")
            result.append(f"   ğŸ“– Read Only: {self.qdrant_settings.read_only}")
            result.append(f"   ğŸ¯ Allow Arbitrary Filter: {self.qdrant_settings.allow_arbitrary_filter}")
            result.append(f"   ğŸ” Permission System: {self.qdrant_settings.enable_permission_system}")
            result.append(f"   ğŸ‘¤ Default Permission: {self.qdrant_settings.default_permission_level}")
            result.append("")
            result.append(f"   ğŸ¤– Embedding Provider: {self.embedding_provider_settings.provider_type}")
            result.append(f"   ğŸ“ Embedding Model: {self.embedding_provider_settings.model_name}")
            result.append(f"   ğŸŒ Ollama Base URL: {self.embedding_provider_settings.base_url}")
            
            return result

        self.tool(
            check_environment_config,
            name="qdrant-check-environment",
            description="æª¢æŸ¥ç³»çµ±çš„ç’°å¢ƒè®Šæ•¸é…ç½®ï¼Œç”¨æ–¼èª¿è©¦å’Œé©—è­‰ .env æ–‡ä»¶æ˜¯å¦æ­£ç¢ºè¼‰å…¥",
        )

        # Collection é…ç½®ç®¡ç†å·¥å…·
        async def list_collection_configs(ctx: Context) -> list[str]:
            """
            åˆ—å‡ºæ‰€æœ‰ collection çš„é…ç½®ä¿¡æ¯
            """
            from mcp_server_qdrant.dynamic_embedding_manager import get_dynamic_embedding_manager
            
            result = ["ğŸ“‹ **Collection é…ç½®åˆ—è¡¨**", ""]
            
            try:
                manager = get_dynamic_embedding_manager()
                configs = manager.list_collection_configs()
                
                if not configs:
                    result.append("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½• collection é…ç½®")
                    return result
                
                for name, config in configs.items():
                    result.append(f"ğŸ“ **{name}**")
                    result.append(f"   ğŸ¤– Provider: {config.embedding_provider.value}")
                    result.append(f"   ğŸ“ Model: {config.embedding_model}")
                    result.append(f"   ğŸ·ï¸ Vector Name: {config.vector_name}")
                    result.append(f"   ğŸ“ Vector Size: {config.vector_size}")
                    if config.ollama_base_url:
                        result.append(f"   ğŸŒ Ollama URL: {config.ollama_base_url}")
                    if config.description:
                        result.append(f"   ğŸ“„ Description: {config.description}")
                    result.append("")
                
                return result
                
            except Exception as e:
                logger.error(f"Failed to list collection configs: {e}")
                return [f"âŒ ç²å–é…ç½®åˆ—è¡¨å¤±æ•—: {str(e)}"]

        async def validate_collection_config(ctx: Context, collection_name: str) -> list[str]:
            """
            é©—è­‰æŒ‡å®š collection çš„é…ç½®å’Œå…¼å®¹æ€§
            """
            from mcp_server_qdrant.dynamic_embedding_manager import get_dynamic_embedding_manager
            
            result = [f"ğŸ” **Collection '{collection_name}' é©—è­‰çµæœ**", ""]
            
            try:
                manager = get_dynamic_embedding_manager()
                validation = manager.validate_collection_compatibility(collection_name)
                
                # åŸºæœ¬ä¿¡æ¯
                result.append("ğŸ“Š **åŸºæœ¬ä¿¡æ¯**")
                result.append(f"   ğŸ“ Collection: {validation['collection_name']}")
                result.append(f"   âš™ï¸ é…ç½®å­˜åœ¨: {'âœ…' if validation['config_exists'] else 'âŒ'}")
                result.append(f"   ğŸ”Œ Provider å¯ç”¨: {'âœ…' if validation['provider_available'] else 'âŒ'}")
                result.append("")
                
                # å‘é‡ä¿¡æ¯
                if validation.get('actual_vector_name'):
                    result.append("ğŸ¯ **å‘é‡ä¿¡æ¯**")
                    result.append(f"   ğŸ·ï¸ Vector Name: {validation['actual_vector_name']} "
                                f"({'âœ…' if validation['vector_name_match'] else 'âŒ'})")
                    result.append(f"   ğŸ“ Vector Size: {validation['actual_vector_size']} "
                                f"({'âœ…' if validation['vector_size_match'] else 'âŒ'})")
                    result.append("")
                
                # è­¦å‘Š
                if validation['warnings']:
                    result.append("âš ï¸ **è­¦å‘Š**")
                    for warning in validation['warnings']:
                        result.append(f"   â€¢ {warning}")
                    result.append("")
                
                # éŒ¯èª¤
                if validation['errors']:
                    result.append("âŒ **éŒ¯èª¤**")
                    for error in validation['errors']:
                        result.append(f"   â€¢ {error}")
                    result.append("")
                
                # ç¸½çµ
                result.append(f"ğŸ“‹ **ç¸½çµ**: {'âœ… é…ç½®æœ‰æ•ˆ' if validation['is_valid'] else 'âŒ é…ç½®æœ‰å•é¡Œ'}")
                
                return result
                
            except Exception as e:
                logger.error(f"Failed to validate collection config: {e}")
                return [f"âŒ é©—è­‰å¤±æ•—: {str(e)}"]

        async def get_collection_detailed_info(ctx: Context, collection_name: str) -> list[str]:
            """
            ç²å– collection çš„è©³ç´°ä¿¡æ¯ï¼ŒåŒ…æ‹¬ Qdrant ç‹€æ…‹å’Œé…ç½®ä¿¡æ¯
            """
            from mcp_server_qdrant.collection_aware_qdrant import CollectionAwareQdrantConnector
            
            result = [f"ğŸ“Š **Collection '{collection_name}' è©³ç´°ä¿¡æ¯**", ""]
            
            try:
                # å‰µå»º collection-aware connector
                connector = CollectionAwareQdrantConnector(
                    qdrant_url=self.qdrant_settings.location,
                    qdrant_api_key=self.qdrant_settings.api_key,
                    qdrant_local_path=self.qdrant_settings.local_path,
                )
                
                # ç²å–è©³ç´°ä¿¡æ¯
                info = await connector.get_collection_info(collection_name)
                
                if info is None:
                    result.append(f"âŒ Collection '{collection_name}' ä¸å­˜åœ¨")
                    return result
                
                # Qdrant çµ±è¨ˆ
                result.append("ğŸ“ˆ **Qdrant çµ±è¨ˆ**")
                result.append(f"   ğŸ“„ Documents: {info['points_count']:,}")
                result.append(f"   ğŸ” Indexed Vectors: {info['indexed_vectors_count']:,}")
                result.append(f"   ğŸ“Š Status: {info['status']}")
                result.append("")
                
                # å‘é‡é…ç½®
                result.append("ğŸ¯ **å‘é‡é…ç½®**")
                for vector_name, vector_config in info['vectors_config'].items():
                    result.append(f"   ğŸ·ï¸ {vector_name}: {vector_config.size}ç¶­, {vector_config.distance}")
                result.append("")
                
                # Embedding é…ç½®
                if 'embedding_config' in info:
                    config = info['embedding_config']
                    result.append("ğŸ¤– **Embedding é…ç½®**")
                    result.append(f"   ğŸ”Œ Provider: {config['provider']}")
                    result.append(f"   ğŸ“ Model: {config['model']}")
                    result.append(f"   ğŸ·ï¸ Vector Name: {config['vector_name']}")
                    result.append(f"   ğŸ“ Vector Size: {config['vector_size']}")
                    result.append("")
                
                return result
                
            except Exception as e:
                logger.error(f"Failed to get collection info: {e}")
                return [f"âŒ ç²å–ä¿¡æ¯å¤±æ•—: {str(e)}"]

        # è¨»å†Š collection ç®¡ç†å·¥å…·
        self.tool(
            list_collection_configs,
            name="qdrant-list-collection-configs",
            description="åˆ—å‡ºæ‰€æœ‰ collection çš„ embedding é…ç½®ä¿¡æ¯",
        )
        
        self.tool(
            validate_collection_config,
            name="qdrant-validate-collection",
            description="é©—è­‰æŒ‡å®š collection çš„é…ç½®å’Œå…¼å®¹æ€§",
        )
        
        self.tool(
            get_collection_detailed_info,
            name="qdrant-collection-info",
            description="ç²å– collection çš„è©³ç´°ä¿¡æ¯ï¼ŒåŒ…æ‹¬ Qdrant ç‹€æ…‹å’Œ embedding é…ç½®",
        )
